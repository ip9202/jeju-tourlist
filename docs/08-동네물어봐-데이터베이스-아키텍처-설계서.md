# "ë™ë„¤ë¬¼ì–´ë´" ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ

## ğŸ“‹ ëª©ì°¨
1. [ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜ ê°œìš”](#-ë°ì´í„°ë² ì´ìŠ¤-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [ë°ì´í„° ëª¨ë¸ë§](#-ë°ì´í„°-ëª¨ë¸ë§)
3. [PostgreSQL ì„¤ê³„](#-postgresql-ì„¤ê³„)
4. [Redis ìºì‹± ì „ëµ](#-redis-ìºì‹±-ì „ëµ)
5. [Elasticsearch ê²€ìƒ‰ ì„¤ê³„](#-elasticsearch-ê²€ìƒ‰-ì„¤ê³„)
6. [ë©€í‹°ë¦¬ì „ í™•ì¥ ì „ëµ](#-ë©€í‹°ë¦¬ì „-í™•ì¥-ì „ëµ)
7. [ì„±ëŠ¥ ìµœì í™”](#-ì„±ëŠ¥-ìµœì í™”)
8. [ë°±ì—… ë° ë³µêµ¬](#-ë°±ì—…-ë°-ë³µêµ¬)
9. [ëª¨ë‹ˆí„°ë§ ë° ìš´ì˜](#-ëª¨ë‹ˆí„°ë§-ë°-ìš´ì˜)
10. [ë°ì´í„° ê±°ë²„ë„ŒìŠ¤](#-ë°ì´í„°-ê±°ë²„ë„ŒìŠ¤)

---

## ğŸ—ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜ ê°œìš”

### ì„¤ê³„ ì›ì¹™

#### 1. ê³„ì¸µë³„ ë°ì´í„° ì €ì¥ ì „ëµ

```mermaid
graph TB
    subgraph "ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆì´ì–´"
        APP[ë™ë„¤ë¬¼ì–´ë´ ì• í”Œë¦¬ì¼€ì´ì…˜]
    end

    subgraph "ìºì‹± ë ˆì´ì–´"
        L1[L1 Cache<br/>ë©”ëª¨ë¦¬ ìºì‹œ<br/>1ë¶„ TTL]
        L2[L2 Cache<br/>Redis<br/>1ì‹œê°„ TTL]
        L3[L3 Cache<br/>CDN<br/>1ì¼ TTL]
    end

    subgraph "ì£¼ ë°ì´í„°ë² ì´ìŠ¤"
        PG_MASTER[PostgreSQL Master<br/>OLTP íŠ¸ëœì­ì…˜]
        PG_SLAVE1[PostgreSQL Slave 1<br/>ì½ê¸° ì „ìš©]
        PG_SLAVE2[PostgreSQL Slave 2<br/>ì½ê¸° ì „ìš©]
    end

    subgraph "ê²€ìƒ‰ ì—”ì§„"
        ES_CLUSTER[Elasticsearch Cluster<br/>ì „ë¬¸ ê²€ìƒ‰ ë° ë¶„ì„]
    end

    subgraph "ê°ì²´ ìŠ¤í† ë¦¬ì§€"
        S3[AWS S3<br/>ì´ë¯¸ì§€ ë° íŒŒì¼]
    end

    subgraph "ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤"
        DW[Data Warehouse<br/>OLAP ë¶„ì„]
    end

    APP --> L1
    L1 --> L2
    L2 --> L3

    APP --> PG_MASTER
    APP --> PG_SLAVE1
    APP --> PG_SLAVE2

    APP --> ES_CLUSTER
    APP --> S3

    PG_MASTER --> PG_SLAVE1
    PG_MASTER --> PG_SLAVE2
    PG_MASTER --> ES_CLUSTER
    PG_MASTER --> DW
```

#### 2. ë°ì´í„° íŠ¹ì„±ë³„ ì €ì¥ ì „ëµ

| ë°ì´í„° ìœ í˜• | ì €ì¥ì†Œ | íŠ¹ì„± | TTL/ë³´ê´€ê¸°ê°„ |
|------------|-------|------|-------------|
| ì‚¬ìš©ì ì„¸ì…˜ | Redis | ë¹ ë¥¸ ì½ê¸°/ì“°ê¸° | 24ì‹œê°„ |
| ì§ˆë¬¸/ë‹µë³€ | PostgreSQL | ACID ë³´ì¥ í•„ìš” | ì˜êµ¬ ë³´ê´€ |
| ê²€ìƒ‰ ì¸ë±ìŠ¤ | Elasticsearch | ì „ë¬¸ ê²€ìƒ‰ ìµœì í™” | ì‹¤ì‹œê°„ ë™ê¸°í™” |
| ì´ë¯¸ì§€/íŒŒì¼ | S3 | í™•ì¥ ê°€ëŠ¥í•œ ê°ì²´ ìŠ¤í† ë¦¬ì§€ | ì˜êµ¬ ë³´ê´€ |
| ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ | Redis + InfluxDB | ì‹œê³„ì—´ ë°ì´í„° | 90ì¼ |
| ë¡œê·¸ ë°ì´í„° | CloudWatch Logs | ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹… | 30ì¼ |

### ë°ì´í„° ì¼ê´€ì„± ëª¨ë¸

#### CAP ì •ë¦¬ ì ìš©

```typescript
// ì„œë¹„ìŠ¤ë³„ ì¼ê´€ì„± ìš”êµ¬ì‚¬í•­
interface ConsistencyRequirement {
  service: string;
  consistency: 'strong' | 'eventual' | 'weak';
  availability: 'high' | 'medium' | 'low';
  partitionTolerance: boolean;
  reasoning: string;
}

const consistencyModels: ConsistencyRequirement[] = [
  {
    service: "user-authentication",
    consistency: "strong",
    availability: "high",
    partitionTolerance: true,
    reasoning: "ì¸ì¦ì€ ì •í™•ì„±ì´ ìµœìš°ì„ , ì´ì¤‘ ë¡œê·¸ì¸ ë°©ì§€ í•„ìš”"
  },
  {
    service: "qa-content",
    consistency: "strong",
    availability: "high",
    partitionTolerance: true,
    reasoning: "ì§ˆë¬¸/ë‹µë³€ì€ ì •í™•í•œ ë‚´ìš© ë³´ì¥ í•„ìš”, ì¤‘ë³µ ë°©ì§€"
  },
  {
    service: "search-results",
    consistency: "eventual",
    availability: "high",
    partitionTolerance: true,
    reasoning: "ê²€ìƒ‰ ê²°ê³¼ëŠ” ì•½ê°„ì˜ ì§€ì—° í—ˆìš©, ê°€ìš©ì„± ìš°ì„ "
  },
  {
    service: "analytics-metrics",
    consistency: "eventual",
    availability: "medium",
    partitionTolerance: true,
    reasoning: "ë¶„ì„ ë°ì´í„°ëŠ” ì •í™•ì„±ë³´ë‹¤ ì§‘ê³„ ì†ë„ ìš°ì„ "
  },
  {
    service: "notification-delivery",
    consistency: "weak",
    availability: "high",
    partitionTolerance: true,
    reasoning: "ì•Œë¦¼ì€ ì†ë„ê°€ ìµœìš°ì„ , ì¼ë¶€ ëˆ„ë½ í—ˆìš©"
  }
];
```

---

## ğŸ“Š ë°ì´í„° ëª¨ë¸ë§

### ì—”í‹°í‹° ê´€ê³„ë„ (ERD)

```mermaid
erDiagram
    USERS {
        uuid id PK
        string email UK
        string password_hash
        string nickname
        string region_code
        boolean is_local_verified
        integer verification_level
        point_balance integer
        json profile_data
        timestamp created_at
        timestamp updated_at
        timestamp last_active_at
    }

    REGIONS {
        string code PK
        string name
        string parent_code FK
        geometry boundary
        json settings
        boolean is_active
    }

    QUESTIONS {
        uuid id PK
        uuid user_id FK
        string title
        text content
        string[] hashtags
        string region_code FK
        string category
        enum urgency
        enum status
        point location
        integer view_count
        integer like_count
        integer answer_count
        uuid accepted_answer_id FK
        timestamp created_at
        timestamp updated_at
    }

    ANSWERS {
        uuid id PK
        uuid question_id FK
        uuid user_id FK
        text content
        string[] images
        boolean is_accepted
        integer like_count
        timestamp accepted_at
        timestamp created_at
        timestamp updated_at
    }

    USER_BADGES {
        uuid id PK
        uuid user_id FK
        string badge_type
        string badge_level
        json metadata
        timestamp earned_at
    }

    POINT_TRANSACTIONS {
        uuid id PK
        uuid user_id FK
        integer amount
        string transaction_type
        string description
        uuid reference_id
        timestamp created_at
    }

    NOTIFICATIONS {
        uuid id PK
        uuid user_id FK
        string type
        string title
        string message
        json data
        boolean is_read
        timestamp created_at
    }

    HASHTAGS {
        uuid id PK
        string name UK
        string normalized_name
        string category
        integer usage_count
        timestamp last_used_at
    }

    USERS ||--o{ QUESTIONS : creates
    USERS ||--o{ ANSWERS : writes
    USERS ||--o{ USER_BADGES : earns
    USERS ||--o{ POINT_TRANSACTIONS : has
    USERS ||--o{ NOTIFICATIONS : receives
    REGIONS ||--o{ USERS : located_in
    REGIONS ||--o{ QUESTIONS : posted_in
    QUESTIONS ||--o{ ANSWERS : has
    QUESTIONS ||--o| ANSWERS : accepted_answer
    HASHTAGS ||--o{ QUESTIONS : tagged_with
```

### ë„ë©”ì¸ë³„ ìƒì„¸ ìŠ¤í‚¤ë§ˆ

#### 1. ì‚¬ìš©ì ë„ë©”ì¸

```sql
-- ì‚¬ìš©ì í…Œì´ë¸”
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255),
    nickname VARCHAR(50) NOT NULL,
    region_code VARCHAR(20) REFERENCES regions(code),

    -- í˜„ì§€ì¸ ì¸ì¦ ì‹œìŠ¤í…œ
    is_local_verified BOOLEAN DEFAULT FALSE,
    verification_level INTEGER DEFAULT 0, -- 0: ë¯¸ì¸ì¦, 1: ì„ì‹œ, 2: ê²€ì¦ë¨, 3: ê³µì‹
    verification_metadata JSONB DEFAULT '{}',

    -- í”„ë¡œí•„ ì •ë³´
    profile_image_url TEXT,
    bio TEXT,
    interested_categories TEXT[],

    -- í¬ì¸íŠ¸ ì‹œìŠ¤í…œ
    point_balance INTEGER DEFAULT 0,
    total_earned_points INTEGER DEFAULT 0,

    -- í™œë™ í†µê³„
    question_count INTEGER DEFAULT 0,
    answer_count INTEGER DEFAULT 0,
    accepted_answer_count INTEGER DEFAULT 0,

    -- ì„¤ì •
    notification_settings JSONB DEFAULT '{}',
    privacy_settings JSONB DEFAULT '{}',

    -- ì†Œì…œ ë¡œê·¸ì¸ ì •ë³´
    social_providers JSONB DEFAULT '{}',

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    last_active_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    -- ì†Œí”„íŠ¸ ì‚­ì œ
    deleted_at TIMESTAMP WITH TIME ZONE,

    -- ì œì•½ ì¡°ê±´
    CONSTRAINT valid_verification_level CHECK (verification_level BETWEEN 0 AND 3),
    CONSTRAINT positive_point_balance CHECK (point_balance >= 0)
);

-- ì¸ë±ìŠ¤ ìµœì í™”
CREATE INDEX idx_users_email ON users(email) WHERE deleted_at IS NULL;
CREATE INDEX idx_users_region_verified ON users(region_code, is_local_verified, last_active_at);
CREATE INDEX idx_users_nickname_trgm ON users USING gin(nickname gin_trgm_ops);
CREATE INDEX idx_users_social_providers ON users USING gin(social_providers);

-- íŠ¸ë¦¬ê±°: ì—…ë°ì´íŠ¸ ì‹œê°„ ìë™ ê°±ì‹ 
CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();
```

#### 2. Q&A ë„ë©”ì¸

```sql
-- ì§ˆë¬¸ í…Œì´ë¸”
CREATE TABLE questions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,

    -- ì½˜í…ì¸ 
    title VARCHAR(200) NOT NULL,
    content TEXT NOT NULL,
    hashtags TEXT[] DEFAULT '{}',

    -- ë¶„ë¥˜
    region_code VARCHAR(20) NOT NULL REFERENCES regions(code),
    category VARCHAR(50) NOT NULL,
    urgency question_urgency DEFAULT 'normal',
    status question_status DEFAULT 'open',

    -- ìœ„ì¹˜ ì •ë³´
    location GEOMETRY(POINT, 4326),
    location_description TEXT,

    -- ìƒí˜¸ì‘ìš© í†µê³„
    view_count INTEGER DEFAULT 0,
    like_count INTEGER DEFAULT 0,
    answer_count INTEGER DEFAULT 0,

    -- ì±„íƒ ì‹œìŠ¤í…œ
    accepted_answer_id UUID REFERENCES answers(id),
    accepted_at TIMESTAMP WITH TIME ZONE,

    -- ì²¨ë¶€ íŒŒì¼
    images TEXT[] DEFAULT '{}',

    -- ë©”íƒ€ë°ì´í„°
    metadata JSONB DEFAULT '{}',

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    -- ì†Œí”„íŠ¸ ì‚­ì œ
    deleted_at TIMESTAMP WITH TIME ZONE,

    -- ì œì•½ ì¡°ê±´
    CONSTRAINT valid_title_length CHECK (LENGTH(title) BETWEEN 10 AND 200),
    CONSTRAINT valid_content_length CHECK (LENGTH(content) BETWEEN 20 AND 2000),
    CONSTRAINT valid_hashtag_count CHECK (array_length(hashtags, 1) <= 5)
);

-- ê¸´ê¸‰ë„ enum
CREATE TYPE question_urgency AS ENUM ('normal', 'urgent', 'emergency');

-- ìƒíƒœ enum
CREATE TYPE question_status AS ENUM ('open', 'answered', 'closed', 'hidden');

-- ë‹µë³€ í…Œì´ë¸”
CREATE TABLE answers (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    question_id UUID NOT NULL REFERENCES questions(id) ON DELETE CASCADE,
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,

    -- ì½˜í…ì¸ 
    content TEXT NOT NULL,
    images TEXT[] DEFAULT '{}',

    -- ìƒí˜¸ì‘ìš©
    like_count INTEGER DEFAULT 0,
    is_accepted BOOLEAN DEFAULT FALSE,

    -- í’ˆì§ˆ ì ìˆ˜ (ML ê¸°ë°˜)
    quality_score FLOAT DEFAULT 0.0,

    -- ë©”íƒ€ë°ì´í„°
    metadata JSONB DEFAULT '{}',

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    accepted_at TIMESTAMP WITH TIME ZONE,

    -- ì†Œí”„íŠ¸ ì‚­ì œ
    deleted_at TIMESTAMP WITH TIME ZONE,

    -- ì œì•½ ì¡°ê±´
    CONSTRAINT valid_content_length CHECK (LENGTH(content) BETWEEN 10 AND 1000),
    CONSTRAINT valid_quality_score CHECK (quality_score BETWEEN 0.0 AND 1.0)
);

-- ë³µí•© ì¸ë±ìŠ¤ ìµœì í™”
CREATE INDEX idx_questions_region_category_created ON questions(region_code, category, created_at DESC);
CREATE INDEX idx_questions_user_status ON questions(user_id, status, created_at DESC);
CREATE INDEX idx_questions_location_gist ON questions USING GIST(location);
CREATE INDEX idx_questions_hashtags_gin ON questions USING GIN(hashtags);
CREATE INDEX idx_questions_search_vector ON questions USING GIN(to_tsvector('korean', title || ' ' || content));

CREATE INDEX idx_answers_question_created ON answers(question_id, created_at DESC);
CREATE INDEX idx_answers_user_accepted ON answers(user_id, is_accepted, created_at DESC);
CREATE INDEX idx_answers_quality_score ON answers(quality_score DESC) WHERE deleted_at IS NULL;
```

#### 3. ì§€ì—­ ë° ìœ„ì¹˜ ë„ë©”ì¸

```sql
-- ì§€ì—­ í…Œì´ë¸” (ê³„ì¸µ êµ¬ì¡°)
CREATE TABLE regions (
    code VARCHAR(20) PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    name_en VARCHAR(100),
    parent_code VARCHAR(20) REFERENCES regions(code),

    -- ì§€ì—­ íƒ€ì…
    region_type region_type NOT NULL,

    -- ì§€ë¦¬ ì •ë³´
    boundary GEOMETRY(MULTIPOLYGON, 4326),
    center_point GEOMETRY(POINT, 4326),

    -- ì„¤ì •
    settings JSONB DEFAULT '{}',
    is_active BOOLEAN DEFAULT TRUE,

    -- í†µê³„
    user_count INTEGER DEFAULT 0,
    question_count INTEGER DEFAULT 0,

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ì§€ì—­ íƒ€ì…
CREATE TYPE region_type AS ENUM ('country', 'province', 'city', 'district', 'neighborhood');

-- ì œì£¼ë„ ì§€ì—­ ë°ì´í„° ì´ˆê¸°í™”
INSERT INTO regions (code, name, name_en, region_type, center_point) VALUES
('KR', 'ëŒ€í•œë¯¼êµ­', 'South Korea', 'country', ST_Point(127.766922, 35.907757)),
('KR-49', 'ì œì£¼íŠ¹ë³„ìì¹˜ë„', 'Jeju Special Self-Governing Province', 'province', ST_Point(126.531188, 33.499621)),
('KR-49-01', 'ì œì£¼ì‹œ', 'Jeju City', 'city', ST_Point(126.521667, 33.513611)),
('KR-49-02', 'ì„œê·€í¬ì‹œ', 'Seogwipo City', 'city', ST_Point(126.555, 33.254167)),
('KR-49-01-01', 'í•œë¦¼ì', 'Hallim-eup', 'district', ST_Point(126.266111, 33.414722)),
('KR-49-01-02', 'ì• ì›”ì', 'Aewol-eup', 'district', ST_Point(126.331944, 33.464722)),
('KR-49-01-03', 'êµ¬ì¢Œì', 'Gujwa-eup', 'district', ST_Point(126.768056, 33.564167)),
('KR-49-01-04', 'ì¡°ì²œì', 'Jocheon-eup', 'district', ST_Point(126.654167, 33.551944)),
('KR-49-02-01', 'ëŒ€ì •ì', 'Daejeong-eup', 'district', ST_Point(126.244722, 33.229167)),
('KR-49-02-02', 'ì•ˆë•ë©´', 'Andeok-myeon', 'district', ST_Point(126.392778, 33.306944)),
('KR-49-02-03', 'í‘œì„ ë©´', 'Pyoseon-myeon', 'district', ST_Point(126.838889, 33.329167)),
('KR-49-02-04', 'ë‚¨ì›ì', 'Namwon-eup', 'district', ST_Point(126.713333, 33.289444)),
('KR-49-02-05', 'ì„±ì‚°ì', 'Seongsan-eup', 'district', ST_Point(126.921944, 33.419444));

-- ê³µê°„ ì¸ë±ìŠ¤
CREATE INDEX idx_regions_boundary ON regions USING GIST(boundary);
CREATE INDEX idx_regions_center_point ON regions USING GIST(center_point);
CREATE INDEX idx_regions_hierarchy ON regions(parent_code, region_type);
```

#### 4. í¬ì¸íŠ¸ ë° ê²Œì„í™” ì‹œìŠ¤í…œ

```sql
-- í¬ì¸íŠ¸ ê±°ë˜ í…Œì´ë¸”
CREATE TABLE point_transactions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,

    -- ê±°ë˜ ì •ë³´
    amount INTEGER NOT NULL,
    transaction_type point_transaction_type NOT NULL,
    description TEXT,

    -- ì°¸ì¡° ì •ë³´
    reference_type VARCHAR(50), -- 'question', 'answer', 'badge' ë“±
    reference_id UUID,

    -- ì”ì•¡ (ìŠ¤ëƒ…ìƒ·)
    balance_before INTEGER NOT NULL,
    balance_after INTEGER NOT NULL,

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    -- ì œì•½ ì¡°ê±´
    CONSTRAINT valid_amount CHECK (amount != 0),
    CONSTRAINT valid_balance_calculation CHECK (balance_after = balance_before + amount)
);

-- í¬ì¸íŠ¸ ê±°ë˜ íƒ€ì…
CREATE TYPE point_transaction_type AS ENUM (
    'earn_answer',           -- ë‹µë³€ ì‘ì„±
    'earn_accepted_answer',  -- ë‹µë³€ ì±„íƒ
    'earn_daily_login',      -- ì¼ì¼ ë¡œê·¸ì¸
    'earn_first_question',   -- ì²« ì§ˆë¬¸ ì‘ì„±
    'earn_milestone',        -- ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„±
    'spend_highlight',       -- ì§ˆë¬¸ ê°•ì¡°
    'spend_premium',         -- í”„ë¦¬ë¯¸ì—„ ê¸°ëŠ¥
    'admin_adjustment'       -- ê´€ë¦¬ì ì¡°ì •
);

-- ë°°ì§€ ì‹œìŠ¤í…œ
CREATE TABLE user_badges (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,

    -- ë°°ì§€ ì •ë³´
    badge_type VARCHAR(50) NOT NULL,
    badge_level INTEGER DEFAULT 1,

    -- ë°°ì§€ ë©”íƒ€ë°ì´í„°
    metadata JSONB DEFAULT '{}',

    -- íƒ€ì„ìŠ¤íƒ¬í”„
    earned_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    -- ìœ ë‹ˆí¬ ì œì•½ (ì‚¬ìš©ìë³„ ë°°ì§€ íƒ€ì… ì¤‘ë³µ ë°©ì§€)
    UNIQUE(user_id, badge_type)
);

-- ì¸ë±ìŠ¤
CREATE INDEX idx_point_transactions_user_created ON point_transactions(user_id, created_at DESC);
CREATE INDEX idx_point_transactions_type ON point_transactions(transaction_type, created_at DESC);
CREATE INDEX idx_user_badges_type ON user_badges(badge_type, earned_at DESC);
```

---

## ğŸ˜ PostgreSQL ì„¤ê³„

### ë¬¼ë¦¬ì  ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„

#### 1. í…Œì´ë¸”ìŠ¤í˜ì´ìŠ¤ ë° íŒŒí‹°ì…”ë‹

```sql
-- í…Œì´ë¸”ìŠ¤í˜ì´ìŠ¤ ìƒì„±
CREATE TABLESPACE ts_questions LOCATION '/var/lib/postgresql/data/questions';
CREATE TABLESPACE ts_answers LOCATION '/var/lib/postgresql/data/answers';
CREATE TABLESPACE ts_logs LOCATION '/var/lib/postgresql/data/logs';
CREATE TABLESPACE ts_analytics LOCATION '/var/lib/postgresql/data/analytics';

-- ë‚ ì§œ ê¸°ë°˜ íŒŒí‹°ì…”ë‹ (ì§ˆë¬¸ í…Œì´ë¸”)
CREATE TABLE questions_partitioned (
    LIKE questions INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- ì›”ë³„ íŒŒí‹°ì…˜ ìƒì„± í•¨ìˆ˜
CREATE OR REPLACE FUNCTION create_monthly_partitions(
    table_name TEXT,
    start_date DATE,
    end_date DATE
) RETURNS VOID AS $$
DECLARE
    current_date DATE := start_date;
    partition_name TEXT;
    partition_start DATE;
    partition_end DATE;
BEGIN
    WHILE current_date < end_date LOOP
        partition_start := DATE_TRUNC('month', current_date);
        partition_end := partition_start + INTERVAL '1 month';
        partition_name := table_name || '_' || TO_CHAR(partition_start, 'YYYY_MM');

        EXECUTE format('CREATE TABLE %I PARTITION OF %I
                       FOR VALUES FROM (%L) TO (%L)',
                      partition_name, table_name, partition_start, partition_end);

        current_date := partition_end;
    END LOOP;
END;
$$ LANGUAGE plpgsql;

-- 2024ë…„ ì „ì²´ íŒŒí‹°ì…˜ ìƒì„±
SELECT create_monthly_partitions('questions_partitioned', '2024-01-01', '2025-01-01');

-- ìë™ íŒŒí‹°ì…˜ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (cron job)
CREATE OR REPLACE FUNCTION auto_create_partitions() RETURNS VOID AS $$
BEGIN
    -- ë‹¤ìŒ 3ê°œì›” íŒŒí‹°ì…˜ ë¯¸ë¦¬ ìƒì„±
    PERFORM create_monthly_partitions(
        'questions_partitioned',
        DATE_TRUNC('month', NOW()),
        DATE_TRUNC('month', NOW()) + INTERVAL '3 months'
    );
END;
$$ LANGUAGE plpgsql;
```

#### 2. ì„±ëŠ¥ ìµœì í™” ì„¤ì •

```sql
-- PostgreSQL ì„±ëŠ¥ ìµœì í™” ì„¤ì •
-- postgresql.conf ê¶Œì¥ ì„¤ì •

-- ë©”ëª¨ë¦¬ ì„¤ì • (16GB RAM ê¸°ì¤€)
shared_buffers = 4GB                    -- RAMì˜ 25%
effective_cache_size = 12GB             -- RAMì˜ 75%
work_mem = 256MB                        -- ì •ë ¬/í•´ì‹œ ì‘ì—…ìš©
maintenance_work_mem = 1GB              -- ì¸ë±ìŠ¤ ìƒì„±/VACUUMìš©

-- ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
checkpoint_completion_target = 0.9      -- ì²´í¬í¬ì¸íŠ¸ ë¶„ì‚°
wal_buffers = 64MB                      -- WAL ë²„í¼ í¬ê¸°
max_wal_size = 4GB                      -- WAL ìµœëŒ€ í¬ê¸°

-- ì—°ê²° ì„¤ì •
max_connections = 200                   -- ìµœëŒ€ ì—°ê²° ìˆ˜
idle_in_transaction_session_timeout = 60000  -- ìœ íœ´ íŠ¸ëœì­ì…˜ íƒ€ì„ì•„ì›ƒ

-- ì¿¼ë¦¬ ìµœì í™”
random_page_cost = 1.1                  -- SSD í™˜ê²½ ìµœì í™”
effective_io_concurrency = 200          -- SSD ë™ì‹œ I/O

-- ë¡œê¹… ì„¤ì •
log_statement = 'mod'                   -- ìˆ˜ì • ì¿¼ë¦¬ë§Œ ë¡œê¹…
log_min_duration_statement = 1000       -- 1ì´ˆ ì´ìƒ ì¿¼ë¦¬ ë¡œê¹…
log_checkpoints = on                    -- ì²´í¬í¬ì¸íŠ¸ ë¡œê¹…
log_connections = on                    -- ì—°ê²° ë¡œê¹…
log_disconnections = on                 -- ì—°ê²° í•´ì œ ë¡œê¹…

-- í†µê³„ ì„¤ì •
track_activity_query_size = 2048        -- ì¿¼ë¦¬ ì¶”ì  í¬ê¸°
track_io_timing = on                    -- I/O íƒ€ì´ë° ì¶”ì 
```

#### 3. ê³ ê¸‰ ì¸ë±ìŠ¤ ì „ëµ

```sql
-- ë³µí•© ì¸ë±ìŠ¤ ì „ëµ
-- 1. ì§ˆë¬¸ ê²€ìƒ‰ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_questions_search_optimized
ON questions (region_code, status, created_at DESC)
WHERE deleted_at IS NULL;

-- 2. ì‚¬ìš©ì í™œë™ ì¡°íšŒ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_questions_user_activity
ON questions (user_id, status)
INCLUDE (title, created_at, answer_count);

-- 3. ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_questions_location_search
ON questions USING GIST (location, region_code)
WHERE status = 'open' AND deleted_at IS NULL;

-- 4. í•´ì‹œíƒœê·¸ ê²€ìƒ‰ ìµœì í™” (GIN ì¸ë±ìŠ¤)
CREATE INDEX CONCURRENTLY idx_questions_hashtags_search
ON questions USING GIN (hashtags)
WHERE status IN ('open', 'answered');

-- 5. ì „ë¬¸ ê²€ìƒ‰ ìµœì í™” (í•œêµ­ì–´ ì§€ì›)
CREATE INDEX CONCURRENTLY idx_questions_fulltext_search
ON questions USING GIN (to_tsvector('korean', title || ' ' || content))
WHERE deleted_at IS NULL;

-- 6. ë¶€ë¶„ ì¸ë±ìŠ¤ í™œìš© (í™œì„± ì‚¬ìš©ìë§Œ)
CREATE INDEX CONCURRENTLY idx_users_active_locals
ON users (region_code, verification_level, last_active_at DESC)
WHERE is_local_verified = true
  AND deleted_at IS NULL
  AND last_active_at > NOW() - INTERVAL '30 days';

-- 7. í•¨ìˆ˜ ê¸°ë°˜ ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_users_nickname_lower
ON users (LOWER(nickname))
WHERE deleted_at IS NULL;

-- 8. ì¡°ê±´ë¶€ ê³ ìœ  ì¸ë±ìŠ¤
CREATE UNIQUE INDEX CONCURRENTLY idx_users_email_unique_active
ON users (email)
WHERE deleted_at IS NULL;
```

#### 4. ì €ì¥ í”„ë¡œì‹œì € ë° í•¨ìˆ˜

```sql
-- ì§ˆë¬¸ ìƒì„± ì‹œ ìë™ ì²˜ë¦¬ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION process_new_question()
RETURNS TRIGGER AS $$
BEGIN
    -- 1. í•´ì‹œíƒœê·¸ ì •ê·œí™” ë° ì €ì¥
    NEW.hashtags := array_agg(DISTINCT normalize_hashtag(hashtag))
    FROM unnest(NEW.hashtags) AS hashtag
    WHERE normalize_hashtag(hashtag) IS NOT NULL;

    -- 2. ì§€ì—­ ì½”ë“œ ìë™ ì„¤ì • (ìœ„ì¹˜ ê¸°ë°˜)
    IF NEW.location IS NOT NULL AND NEW.region_code IS NULL THEN
        SELECT code INTO NEW.region_code
        FROM regions
        WHERE ST_Contains(boundary, NEW.location)
          AND region_type = 'district'
        ORDER BY ST_Area(boundary)
        LIMIT 1;
    END IF;

    -- 3. ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ML ê¸°ë°˜)
    IF NEW.category IS NULL THEN
        NEW.category := classify_question_category(NEW.title, NEW.content);
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- íŠ¸ë¦¬ê±° ë“±ë¡
CREATE TRIGGER trigger_process_new_question
    BEFORE INSERT ON questions
    FOR EACH ROW
    EXECUTE FUNCTION process_new_question();

-- ë‹µë³€ ì±„íƒ ì‹œ í¬ì¸íŠ¸ ì§€ê¸‰ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION award_points_for_accepted_answer()
RETURNS TRIGGER AS $$
DECLARE
    answerer_id UUID;
    questioner_id UUID;
    points_to_award INTEGER := 150; -- ê¸°ë³¸ 150 í¬ì¸íŠ¸
BEGIN
    -- ë‹µë³€ì´ ì±„íƒë˜ì—ˆì„ ë•Œë§Œ ì‹¤í–‰
    IF NEW.is_accepted = TRUE AND OLD.is_accepted = FALSE THEN
        -- ë‹µë³€ìì™€ ì§ˆë¬¸ì ID ì¡°íšŒ
        SELECT a.user_id, q.user_id
        INTO answerer_id, questioner_id
        FROM answers a
        JOIN questions q ON a.question_id = q.id
        WHERE a.id = NEW.id;

        -- ë‹µë³€ìì—ê²Œ í¬ì¸íŠ¸ ì§€ê¸‰
        INSERT INTO point_transactions (
            user_id, amount, transaction_type, description,
            reference_type, reference_id, balance_before, balance_after
        ) VALUES (
            answerer_id, points_to_award, 'earn_accepted_answer',
            'ë‹µë³€ ì±„íƒìœ¼ë¡œ ì¸í•œ í¬ì¸íŠ¸ íšë“',
            'answer', NEW.id,
            (SELECT point_balance FROM users WHERE id = answerer_id),
            (SELECT point_balance FROM users WHERE id = answerer_id) + points_to_award
        );

        -- ì‚¬ìš©ì í¬ì¸íŠ¸ ì”ì•¡ ì—…ë°ì´íŠ¸
        UPDATE users
        SET point_balance = point_balance + points_to_award,
            total_earned_points = total_earned_points + points_to_award,
            accepted_answer_count = accepted_answer_count + 1
        WHERE id = answerer_id;

        -- ì§ˆë¬¸ì˜ ì±„íƒ ë‹µë³€ ID ì—…ë°ì´íŠ¸
        UPDATE questions
        SET accepted_answer_id = NEW.id,
            accepted_at = NOW(),
            status = 'answered'
        WHERE id = NEW.question_id;

        -- ì•Œë¦¼ ë°œì†¡ íì— ì¶”ê°€
        INSERT INTO notification_queue (
            user_id, type, title, message, data
        ) VALUES (
            answerer_id, 'answer_accepted',
            'ë‹µë³€ì´ ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤!',
            format('"%s" ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì±„íƒë˜ì–´ %s í¬ì¸íŠ¸ë¥¼ íšë“í–ˆìŠµë‹ˆë‹¤.',
                   (SELECT title FROM questions WHERE id = NEW.question_id),
                   points_to_award),
            json_build_object('answer_id', NEW.id, 'points', points_to_award)
        );
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- íŠ¸ë¦¬ê±° ë“±ë¡
CREATE TRIGGER trigger_award_points_accepted_answer
    AFTER UPDATE ON answers
    FOR EACH ROW
    EXECUTE FUNCTION award_points_for_accepted_answer();
```

---

## âš¡ Redis ìºì‹± ì „ëµ

### Redis í´ëŸ¬ìŠ¤í„° ì„¤ê³„

#### 1. í´ëŸ¬ìŠ¤í„° êµ¬ì„±

```yaml
# Redis Cluster ì„¤ì •
version: '3.8'
services:
  redis-node-1:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf
    ports:
      - "7001:7001"
      - "17001:17001"
    volumes:
      - ./redis-cluster.conf:/etc/redis/redis.conf
      - redis-node-1-data:/data
    environment:
      - REDIS_PORT=7001

  redis-node-2:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf
    ports:
      - "7002:7002"
      - "17002:17002"
    volumes:
      - ./redis-cluster.conf:/etc/redis/redis.conf
      - redis-node-2-data:/data
    environment:
      - REDIS_PORT=7002

  redis-node-3:
    image: redis:7-alpine
    command: redis-server /etc/redis/redis.conf
    ports:
      - "7003:7003"
      - "17003:17003"
    volumes:
      - ./redis-cluster.conf:/etc/redis/redis.conf
      - redis-node-3-data:/data
    environment:
      - REDIS_PORT=7003

volumes:
  redis-node-1-data:
  redis-node-2-data:
  redis-node-3-data:
```

```conf
# redis-cluster.conf
# í´ëŸ¬ìŠ¤í„° ëª¨ë“œ í™œì„±í™”
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 15000
cluster-require-full-coverage no

# ë„¤íŠ¸ì›Œí¬ ì„¤ì •
bind 0.0.0.0
port 7001
cluster-announce-ip 127.0.0.1
cluster-announce-port 7001
cluster-announce-bus-port 17001

# ë©”ëª¨ë¦¬ ì„¤ì •
maxmemory 1gb
maxmemory-policy allkeys-lru

# ì˜ì†ì„± ì„¤ì •
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec

# ë³´ì•ˆ ì„¤ì •
requirepass your_redis_password
masterauth your_redis_password

# ì„±ëŠ¥ ìµœì í™”
tcp-keepalive 60
timeout 300
```

#### 2. ìºì‹± íŒ¨í„´ êµ¬í˜„

```typescript
// Redis ìºì‹± ë§¤ë‹ˆì €
export class RedisCacheManager {
  private cluster: Cluster;
  private localCache: NodeCache;

  constructor() {
    this.cluster = new Redis.Cluster([
      { host: 'redis-node-1', port: 7001 },
      { host: 'redis-node-2', port: 7002 },
      { host: 'redis-node-3', port: 7003 }
    ], {
      redisOptions: {
        password: process.env.REDIS_PASSWORD,
        connectTimeout: 60000,
        maxRetriesPerRequest: 3
      },
      enableOfflineQueue: false,
      retryDelayOnFailover: 100,
      maxRetriesPerRequest: 3
    });

    // L1 ìºì‹œ (ë©”ëª¨ë¦¬)
    this.localCache = new NodeCache({
      stdTTL: 60,           // 1ë¶„ ê¸°ë³¸ TTL
      checkperiod: 120,     // 2ë¶„ë§ˆë‹¤ ë§Œë£Œ ì²´í¬
      maxKeys: 1000         // ìµœëŒ€ 1000ê°œ í‚¤
    });
  }

  // ê³„ì¸µë³„ ìºì‹± ì „ëµ
  async get<T>(key: string): Promise<T | null> {
    // L1 ìºì‹œ í™•ì¸
    const localValue = this.localCache.get<T>(key);
    if (localValue !== undefined) {
      return localValue;
    }

    // L2 ìºì‹œ (Redis) í™•ì¸
    try {
      const redisValue = await this.cluster.get(key);
      if (redisValue) {
        const parsedValue = JSON.parse(redisValue) as T;

        // L1 ìºì‹œì— ì €ì¥ (ë” ì§§ì€ TTL)
        this.localCache.set(key, parsedValue, 30);

        return parsedValue;
      }
    } catch (error) {
      console.error('Redis get error:', error);
    }

    return null;
  }

  async set<T>(key: string, value: T, ttl: number = 3600): Promise<void> {
    try {
      // L2 ìºì‹œ (Redis)ì— ì €ì¥
      await this.cluster.setex(key, ttl, JSON.stringify(value));

      // L1 ìºì‹œì—ë„ ì €ì¥ (ë” ì§§ì€ TTL)
      this.localCache.set(key, value, Math.min(ttl, 60));
    } catch (error) {
      console.error('Redis set error:', error);
    }
  }

  // Cache-Aside íŒ¨í„´
  async getOrSet<T>(
    key: string,
    fetchFunction: () => Promise<T>,
    ttl: number = 3600
  ): Promise<T> {
    // ìºì‹œì—ì„œ ì¡°íšŒ
    const cached = await this.get<T>(key);
    if (cached !== null) {
      return cached;
    }

    // ìºì‹œ ë¯¸ìŠ¤ ì‹œ DBì—ì„œ ì¡°íšŒ
    const value = await fetchFunction();

    // ìºì‹œì— ì €ì¥
    await this.set(key, value, ttl);

    return value;
  }

  // Write-Through íŒ¨í„´
  async setWithWriteThrough<T>(
    key: string,
    value: T,
    dbWriteFunction: (value: T) => Promise<void>,
    ttl: number = 3600
  ): Promise<void> {
    // 1. DBì— ë¨¼ì € ì €ì¥
    await dbWriteFunction(value);

    // 2. ìºì‹œì— ì €ì¥
    await this.set(key, value, ttl);
  }

  // íŒ¨í„´ë³„ ë¬´íš¨í™”
  async invalidatePattern(pattern: string): Promise<void> {
    try {
      // L1 ìºì‹œ ì „ì²´ í´ë¦¬ì–´ (íŒ¨í„´ ë§¤ì¹­ ì–´ë ¤ì›€)
      this.localCache.flushAll();

      // L2 ìºì‹œ íŒ¨í„´ ë§¤ì¹­ ì‚­ì œ
      const keys = await this.cluster.keys(pattern);
      if (keys.length > 0) {
        await this.cluster.del(...keys);
      }
    } catch (error) {
      console.error('Cache invalidation error:', error);
    }
  }
}

// ë„ë©”ì¸ë³„ ìºì‹± ì „ëµ
export class DomainCacheStrategies {
  constructor(private cacheManager: RedisCacheManager) {}

  // ì‚¬ìš©ì í”„ë¡œí•„ ìºì‹±
  async getUserProfile(userId: string): Promise<UserProfile | null> {
    const key = `user:profile:${userId}`;

    return this.cacheManager.getOrSet(
      key,
      async () => {
        const user = await User.findById(userId);
        return user ? this.transformToProfile(user) : null;
      },
      3600 // 1ì‹œê°„ ìºì‹œ
    );
  }

  // ì§ˆë¬¸ ëª©ë¡ ìºì‹± (í˜ì´ì§€ë„¤ì´ì…˜)
  async getQuestionList(params: QuestionListParams): Promise<PaginatedQuestions> {
    const key = `questions:list:${this.hashParams(params)}`;

    return this.cacheManager.getOrSet(
      key,
      async () => await QuestionService.getQuestions(params),
      600 // 10ë¶„ ìºì‹œ
    );
  }

  // íŠ¸ë Œë”© í•´ì‹œíƒœê·¸ ìºì‹±
  async getTrendingHashtags(): Promise<TrendingTag[]> {
    const key = 'trending:hashtags:24h';

    return this.cacheManager.getOrSet(
      key,
      async () => await HashtagService.calculateTrending(),
      900 // 15ë¶„ ìºì‹œ
    );
  }

  // ì§€ì—­ë³„ í†µê³„ ìºì‹±
  async getRegionStats(regionCode: string): Promise<RegionStats> {
    const key = `region:stats:${regionCode}`;

    return this.cacheManager.getOrSet(
      key,
      async () => await RegionService.calculateStats(regionCode),
      1800 // 30ë¶„ ìºì‹œ
    );
  }

  // ì‹¤ì‹œê°„ ì•Œë¦¼ ì¹´ìš´íŠ¸ ìºì‹±
  async getNotificationCount(userId: string): Promise<number> {
    const key = `notifications:count:${userId}`;

    const cached = await this.cacheManager.get<number>(key);
    if (cached !== null) return cached;

    const count = await NotificationService.getUnreadCount(userId);
    await this.cacheManager.set(key, count, 300); // 5ë¶„ ìºì‹œ

    return count;
  }

  // ìºì‹œ ì›Œë° (ì‚¬ì „ ë¡œë”©)
  async warmUpCache(): Promise<void> {
    const warmUpTasks = [
      // ì¸ê¸° ì§€ì—­ ë°ì´í„° ì‚¬ì „ ë¡œë”©
      this.getRegionStats('KR-49'), // ì œì£¼ë„
      this.getRegionStats('KR-49-01'), // ì œì£¼ì‹œ
      this.getRegionStats('KR-49-02'), // ì„œê·€í¬ì‹œ

      // íŠ¸ë Œë”© ë°ì´í„° ì‚¬ì „ ë¡œë”©
      this.getTrendingHashtags(),

      // ì¸ê¸° ì§ˆë¬¸ ëª©ë¡ ì‚¬ì „ ë¡œë”©
      this.getQuestionList({
        region: 'KR-49',
        sortBy: 'popularity',
        page: 1
      })
    ];

    await Promise.allSettled(warmUpTasks);
    console.log('Cache warm-up completed');
  }

  private hashParams(params: any): string {
    return crypto
      .createHash('md5')
      .update(JSON.stringify(params))
      .digest('hex');
  }

  private transformToProfile(user: User): UserProfile {
    return {
      id: user.id,
      nickname: user.nickname,
      regionCode: user.region_code,
      isLocalVerified: user.is_local_verified,
      verificationLevel: user.verification_level,
      pointBalance: user.point_balance,
      badges: user.badges?.slice(0, 5) || [], // ìµœëŒ€ 5ê°œë§Œ
      stats: {
        questionCount: user.question_count,
        answerCount: user.answer_count,
        acceptedAnswerCount: user.accepted_answer_count
      }
    };
  }
}
```

#### 3. ì‹¤ì‹œê°„ ë°ì´í„° ìºì‹±

```typescript
// ì‹¤ì‹œê°„ ì„¸ì…˜ ê´€ë¦¬
export class RealtimeSessionCache {
  private readonly SESSION_TTL = 86400; // 24ì‹œê°„
  private readonly ACTIVITY_TTL = 1800;  // 30ë¶„

  constructor(private redis: RedisCacheManager) {}

  // ì‚¬ìš©ì ì˜¨ë¼ì¸ ìƒíƒœ ê´€ë¦¬
  async setUserOnline(userId: string, socketId: string): Promise<void> {
    const pipeline = this.redis.cluster.pipeline();

    // ì‚¬ìš©ì ì„¸ì…˜ ì €ì¥
    pipeline.hset(`session:${userId}`, {
      socketId,
      status: 'online',
      connectedAt: Date.now(),
      lastActivity: Date.now()
    });
    pipeline.expire(`session:${userId}`, this.SESSION_TTL);

    // ì˜¨ë¼ì¸ ì‚¬ìš©ì ëª©ë¡ì— ì¶”ê°€
    pipeline.sadd('users:online', userId);
    pipeline.expire('users:online', this.ACTIVITY_TTL);

    // ì§€ì—­ë³„ ì˜¨ë¼ì¸ ì‚¬ìš©ì ì¶”ê°€
    const user = await this.getUserRegion(userId);
    if (user?.regionCode) {
      pipeline.sadd(`users:online:${user.regionCode}`, userId);
      pipeline.expire(`users:online:${user.regionCode}`, this.ACTIVITY_TTL);
    }

    await pipeline.exec();
  }

  async setUserOffline(userId: string): Promise<void> {
    const pipeline = this.redis.cluster.pipeline();

    // ì„¸ì…˜ ì‚­ì œ
    pipeline.del(`session:${userId}`);

    // ì˜¨ë¼ì¸ ëª©ë¡ì—ì„œ ì œê±°
    pipeline.srem('users:online', userId);

    // ì§€ì—­ë³„ ì˜¨ë¼ì¸ ëª©ë¡ì—ì„œë„ ì œê±°
    const user = await this.getUserRegion(userId);
    if (user?.regionCode) {
      pipeline.srem(`users:online:${user.regionCode}`, userId);
    }

    await pipeline.exec();
  }

  // í™œë™ ì¶”ì 
  async updateUserActivity(userId: string): Promise<void> {
    await this.redis.cluster.hset(
      `session:${userId}`,
      'lastActivity',
      Date.now()
    );
  }

  // ì§€ì—­ë³„ ì˜¨ë¼ì¸ ì‚¬ìš©ì ì¡°íšŒ
  async getOnlineUsersInRegion(regionCode: string): Promise<string[]> {
    return this.redis.cluster.smembers(`users:online:${regionCode}`);
  }

  // íƒ€ì´í•‘ ìƒíƒœ ê´€ë¦¬
  async setUserTyping(questionId: string, userId: string): Promise<void> {
    const key = `typing:${questionId}`;

    await this.redis.cluster.hset(key, userId, Date.now());
    await this.redis.cluster.expire(key, 10); // 10ì´ˆ í›„ ìë™ ë§Œë£Œ
  }

  async removeUserTyping(questionId: string, userId: string): Promise<void> {
    await this.redis.cluster.hdel(`typing:${questionId}`, userId);
  }

  async getTypingUsers(questionId: string): Promise<string[]> {
    const typingData = await this.redis.cluster.hgetall(`typing:${questionId}`);
    const now = Date.now();
    const activeUsers: string[] = [];

    for (const [userId, timestamp] of Object.entries(typingData)) {
      if (now - parseInt(timestamp) < 5000) { // 5ì´ˆ ì´ë‚´ í™œë™
        activeUsers.push(userId);
      }
    }

    return activeUsers;
  }

  // ì‹¤ì‹œê°„ í†µê³„ ìºì‹±
  async incrementQuestionViews(questionId: string): Promise<number> {
    const key = `stats:question:${questionId}:views`;
    const count = await this.redis.cluster.incr(key);

    // 1ì‹œê°„ë§ˆë‹¤ DBì— ë™ê¸°í™”
    if (count % 10 === 0) { // 10íšŒë§ˆë‹¤ DB ì—…ë°ì´íŠ¸
      await this.syncViewsToDB(questionId, count);
    }

    return count;
  }

  private async syncViewsToDB(questionId: string, views: number): Promise<void> {
    try {
      await Question.update(
        { id: questionId },
        { view_count: views }
      );
    } catch (error) {
      console.error('Failed to sync views to DB:', error);
    }
  }

  private async getUserRegion(userId: string): Promise<{ regionCode: string } | null> {
    // ìºì‹œì—ì„œ ì‚¬ìš©ì ì§€ì—­ ì •ë³´ ì¡°íšŒ
    const key = `user:region:${userId}`;
    let regionCode = await this.redis.get<string>(key);

    if (!regionCode) {
      const user = await User.findById(userId);
      regionCode = user?.region_code || null;

      if (regionCode) {
        await this.redis.set(key, regionCode, 3600); // 1ì‹œê°„ ìºì‹œ
      }
    }

    return regionCode ? { regionCode } : null;
  }
}
```

---

## ğŸ” Elasticsearch ê²€ìƒ‰ ì„¤ê³„

### ì¸ë±ìŠ¤ ì„¤ê³„ ë° ë§¤í•‘

#### 1. ì§ˆë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤

```json
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 1,
    "analysis": {
      "analyzer": {
        "korean_analyzer": {
          "type": "custom",
          "tokenizer": "nori_tokenizer",
          "filter": [
            "nori_part_of_speech",
            "nori_readingform",
            "lowercase",
            "stop",
            "synonym_filter"
          ]
        },
        "hashtag_analyzer": {
          "type": "custom",
          "tokenizer": "keyword",
          "filter": ["lowercase", "trim"]
        },
        "ngram_analyzer": {
          "type": "custom",
          "tokenizer": "ngram_tokenizer",
          "filter": ["lowercase"]
        }
      },
      "tokenizer": {
        "nori_tokenizer": {
          "type": "nori_tokenizer",
          "decompound_mode": "mixed",
          "user_dictionary": "user_dict.txt"
        },
        "ngram_tokenizer": {
          "type": "ngram",
          "min_gram": 2,
          "max_gram": 3,
          "token_chars": ["letter", "digit"]
        }
      },
      "filter": {
        "nori_part_of_speech": {
          "type": "nori_part_of_speech",
          "stoptags": ["E", "IC", "J", "MAG", "MAJ", "MM", "SP", "SSC", "SSO", "SC", "SE", "XPN", "XSA", "XSN", "XSV", "UNA", "NA", "VSV"]
        },
        "synonym_filter": {
          "type": "synonym",
          "synonyms": [
            "ë§›ì§‘,ë§›ìˆëŠ”ì§‘,ì‹ë‹¹,ë ˆìŠ¤í† ë‘",
            "ì¹´í˜,ì»¤í”¼ìˆ,ë‹¤ë°©",
            "ìˆ™ì†Œ,íœì…˜,í˜¸í…”,ë¦¬ì¡°íŠ¸",
            "í•´ë³€,ë°”ë‹¤,í•´ìˆ˜ìš•ì¥",
            "ì‚°,ì˜¤ë¦„,ë“±ì‚°"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "id": {
        "type": "keyword"
      },
      "title": {
        "type": "text",
        "analyzer": "korean_analyzer",
        "fields": {
          "ngram": {
            "type": "text",
            "analyzer": "ngram_analyzer"
          },
          "keyword": {
            "type": "keyword"
          }
        }
      },
      "content": {
        "type": "text",
        "analyzer": "korean_analyzer"
      },
      "hashtags": {
        "type": "text",
        "analyzer": "hashtag_analyzer",
        "fields": {
          "keyword": {
            "type": "keyword"
          }
        }
      },
      "region_code": {
        "type": "keyword"
      },
      "category": {
        "type": "keyword"
      },
      "urgency": {
        "type": "keyword"
      },
      "status": {
        "type": "keyword"
      },
      "location": {
        "type": "geo_point"
      },
      "user": {
        "properties": {
          "id": { "type": "keyword" },
          "nickname": {
            "type": "text",
            "analyzer": "korean_analyzer"
          },
          "is_local_verified": { "type": "boolean" },
          "verification_level": { "type": "integer" }
        }
      },
      "stats": {
        "properties": {
          "view_count": { "type": "integer" },
          "like_count": { "type": "integer" },
          "answer_count": { "type": "integer" }
        }
      },
      "created_at": {
        "type": "date"
      },
      "updated_at": {
        "type": "date"
      },
      "popularity_score": {
        "type": "float"
      }
    }
  }
}
```

#### 2. ê²€ìƒ‰ ì„œë¹„ìŠ¤ êµ¬í˜„

```typescript
// Elasticsearch ê²€ìƒ‰ ì„œë¹„ìŠ¤
export class ElasticsearchService {
  private client: Client;

  constructor() {
    this.client = new Client({
      node: process.env.ELASTICSEARCH_URL || 'http://localhost:9200',
      auth: {
        username: process.env.ES_USERNAME || 'elastic',
        password: process.env.ES_PASSWORD || 'password'
      },
      requestTimeout: 60000,
      maxRetries: 3
    });
  }

  // í†µí•© ê²€ìƒ‰
  async searchQuestions(params: SearchParams): Promise<SearchResult> {
    const {
      query,
      filters = {},
      sort = 'relevance',
      page = 1,
      size = 20,
      location
    } = params;

    const searchBody: any = {
      query: this.buildSearchQuery(query, filters, location),
      sort: this.buildSortClause(sort),
      highlight: {
        fields: {
          title: { number_of_fragments: 0 },
          content: { fragment_size: 150, number_of_fragments: 3 }
        },
        pre_tags: ['<mark>'],
        post_tags: ['</mark>']
      },
      aggs: {
        categories: {
          terms: { field: 'category', size: 10 }
        },
        regions: {
          terms: { field: 'region_code', size: 20 }
        },
        hashtags: {
          terms: { field: 'hashtags.keyword', size: 30 }
        }
      },
      from: (page - 1) * size,
      size
    };

    try {
      const response = await this.client.search({
        index: 'questions',
        body: searchBody
      });

      return this.formatSearchResponse(response.body);
    } catch (error) {
      console.error('Elasticsearch search error:', error);
      throw new Error('Search service unavailable');
    }
  }

  private buildSearchQuery(query: string, filters: SearchFilters, location?: LocationFilter): any {
    const must: any[] = [];
    const filter: any[] = [];

    // í…ìŠ¤íŠ¸ ê²€ìƒ‰
    if (query) {
      must.push({
        multi_match: {
          query,
          fields: [
            'title^3',           // ì œëª©ì— ë†’ì€ ê°€ì¤‘ì¹˜
            'title.ngram^2',     // ë¶€ë¶„ ì¼ì¹˜
            'content^1',         // ë‚´ìš©
            'hashtags^2',        // í•´ì‹œíƒœê·¸
            'user.nickname^1'    // ì‚¬ìš©ìëª…
          ],
          type: 'best_fields',
          fuzziness: 'AUTO',     // ìë™ ì˜¤íƒ€ ë³´ì •
          operator: 'and'
        }
      });
    } else {
      must.push({ match_all: {} });
    }

    // í•„í„° ì¡°ê±´
    filter.push({ term: { status: 'open' } }); // í™œì„± ì§ˆë¬¸ë§Œ

    if (filters.regionCode) {
      filter.push({ term: { region_code: filters.regionCode } });
    }

    if (filters.category) {
      filter.push({ term: { category: filters.category } });
    }

    if (filters.hashtags?.length > 0) {
      filter.push({
        terms: { 'hashtags.keyword': filters.hashtags }
      });
    }

    if (filters.urgency) {
      filter.push({ term: { urgency: filters.urgency } });
    }

    if (filters.hasAnswers !== undefined) {
      if (filters.hasAnswers) {
        filter.push({ range: { 'stats.answer_count': { gt: 0 } } });
      } else {
        filter.push({ term: { 'stats.answer_count': 0 } });
      }
    }

    if (filters.localOnly) {
      filter.push({ term: { 'user.is_local_verified': true } });
    }

    // ë‚ ì§œ í•„í„°
    if (filters.dateRange) {
      filter.push({
        range: {
          created_at: {
            gte: filters.dateRange.from,
            lte: filters.dateRange.to
          }
        }
      });
    }

    // ìœ„ì¹˜ ê¸°ë°˜ ê²€ìƒ‰
    if (location) {
      filter.push({
        geo_distance: {
          distance: `${location.radius}km`,
          location: {
            lat: location.lat,
            lon: location.lng
          }
        }
      });
    }

    return {
      bool: {
        must,
        filter
      }
    };
  }

  private buildSortClause(sort: string): any[] {
    switch (sort) {
      case 'newest':
        return [{ created_at: { order: 'desc' } }];

      case 'oldest':
        return [{ created_at: { order: 'asc' } }];

      case 'popularity':
        return [
          { popularity_score: { order: 'desc' } },
          { created_at: { order: 'desc' } }
        ];

      case 'most_answers':
        return [
          { 'stats.answer_count': { order: 'desc' } },
          { created_at: { order: 'desc' } }
        ];

      case 'most_views':
        return [
          { 'stats.view_count': { order: 'desc' } },
          { created_at: { order: 'desc' } }
        ];

      case 'urgent':
        return [
          {
            _script: {
              type: 'number',
              script: {
                source: "params.urgency_scores[doc['urgency'].value] ?: 0",
                params: {
                  urgency_scores: {
                    emergency: 3,
                    urgent: 2,
                    normal: 1
                  }
                }
              },
              order: 'desc'
            }
          },
          { created_at: { order: 'desc' } }
        ];

      case 'relevance':
      default:
        return ['_score', { created_at: { order: 'desc' } }];
    }
  }

  // ìë™ì™„ì„± ê¸°ëŠ¥
  async getAutoSuggestions(query: string, size: number = 5): Promise<string[]> {
    try {
      const response = await this.client.search({
        index: 'questions',
        body: {
          suggest: {
            title_suggest: {
              prefix: query,
              completion: {
                field: 'title.suggest',
                size,
                contexts: {
                  region: ['KR-49'] // ì œì£¼ë„ ì¤‘ì‹¬ ì œì•ˆ
                }
              }
            }
          },
          _source: false
        }
      });

      return response.body.suggest.title_suggest[0].options
        .map((option: any) => option.text);
    } catch (error) {
      console.error('Auto-suggest error:', error);
      return [];
    }
  }

  // íŠ¸ë Œë”© í‚¤ì›Œë“œ ë¶„ì„
  async getTrendingKeywords(timeRange: string = '7d'): Promise<TrendingKeyword[]> {
    try {
      const response = await this.client.search({
        index: 'questions',
        body: {
          query: {
            range: {
              created_at: {
                gte: `now-${timeRange}`
              }
            }
          },
          aggs: {
            trending_hashtags: {
              terms: {
                field: 'hashtags.keyword',
                size: 20,
                order: { _count: 'desc' }
              }
            },
            trending_categories: {
              terms: {
                field: 'category',
                size: 10,
                order: { _count: 'desc' }
              }
            }
          },
          size: 0
        }
      });

      const hashtags = response.body.aggregations.trending_hashtags.buckets
        .map((bucket: any) => ({
          keyword: bucket.key,
          count: bucket.doc_count,
          type: 'hashtag'
        }));

      const categories = response.body.aggregations.trending_categories.buckets
        .map((bucket: any) => ({
          keyword: bucket.key,
          count: bucket.doc_count,
          type: 'category'
        }));

      return [...hashtags, ...categories]
        .sort((a, b) => b.count - a.count)
        .slice(0, 15);
    } catch (error) {
      console.error('Trending keywords error:', error);
      return [];
    }
  }

  // ë°ì´í„° ë™ê¸°í™”
  async syncQuestionToES(question: Question): Promise<void> {
    const doc = {
      id: question.id,
      title: question.title,
      content: question.content,
      hashtags: question.hashtags,
      region_code: question.region_code,
      category: question.category,
      urgency: question.urgency,
      status: question.status,
      location: question.location ? {
        lat: question.location.lat,
        lon: question.location.lng
      } : null,
      user: {
        id: question.user.id,
        nickname: question.user.nickname,
        is_local_verified: question.user.is_local_verified,
        verification_level: question.user.verification_level
      },
      stats: {
        view_count: question.view_count || 0,
        like_count: question.like_count || 0,
        answer_count: question.answer_count || 0
      },
      created_at: question.created_at,
      updated_at: question.updated_at,
      popularity_score: this.calculatePopularityScore(question)
    };

    try {
      await this.client.index({
        index: 'questions',
        id: question.id,
        body: doc
      });
    } catch (error) {
      console.error('ES sync error:', error);
    }
  }

  private calculatePopularityScore(question: Question): number {
    const ageInHours = (Date.now() - new Date(question.created_at).getTime()) / (1000 * 60 * 60);
    const viewWeight = (question.view_count || 0) * 0.1;
    const answerWeight = (question.answer_count || 0) * 2;
    const likeWeight = (question.like_count || 0) * 1;
    const urgencyWeight = question.urgency === 'urgent' ? 5 : question.urgency === 'emergency' ? 10 : 0;

    // ì‹œê°„ ê°ì‡  ì ìš© (24ì‹œê°„ ê¸°ì¤€ ë°˜ê°ê¸°)
    const timeDecay = Math.pow(0.5, ageInHours / 24);

    return (viewWeight + answerWeight + likeWeight + urgencyWeight) * timeDecay;
  }

  private formatSearchResponse(response: any): SearchResult {
    return {
      hits: response.hits.hits.map((hit: any) => ({
        ...hit._source,
        highlight: hit.highlight,
        score: hit._score
      })),
      total: response.hits.total.value,
      aggregations: {
        categories: response.aggregations?.categories?.buckets || [],
        regions: response.aggregations?.regions?.buckets || [],
        hashtags: response.aggregations?.hashtags?.buckets || []
      },
      took: response.took
    };
  }
}
```

---

## ğŸŒ ë©€í‹°ë¦¬ì „ í™•ì¥ ì „ëµ

### ë°ì´í„° ìƒ¤ë”© ì„¤ê³„

#### 1. ì§€ì—­ ê¸°ë°˜ ìƒ¤ë”©

```typescript
// ì§€ì—­ ê¸°ë°˜ ìƒ¤ë”© ë§¤ë‹ˆì €
export class RegionShardManager {
  private shards: Map<string, DatabaseShard> = new Map();
  private defaultShard: string = 'jeju';

  constructor() {
    this.initializeShards();
  }

  private initializeShards(): void {
    const shardConfigs: ShardConfig[] = [
      {
        shardId: 'jeju',
        regions: ['KR-49'],
        primary: {
          host: 'jeju-primary.db.internal',
          port: 5432,
          database: 'dongne_jeju'
        },
        replicas: [
          {
            host: 'jeju-replica-1.db.internal',
            port: 5432,
            database: 'dongne_jeju'
          },
          {
            host: 'jeju-replica-2.db.internal',
            port: 5432,
            database: 'dongne_jeju'
          }
        ],
        isActive: true
      },
      {
        shardId: 'busan',
        regions: ['KR-26'],
        primary: {
          host: 'busan-primary.db.internal',
          port: 5432,
          database: 'dongne_busan'
        },
        replicas: [
          {
            host: 'busan-replica-1.db.internal',
            port: 5432,
            database: 'dongne_busan'
          }
        ],
        isActive: false // í–¥í›„ í™œì„±í™”
      }
    ];

    shardConfigs.forEach(config => {
      this.shards.set(config.shardId, new DatabaseShard(config));
    });
  }

  // ì§€ì—­ ì½”ë“œ ê¸°ë°˜ ìƒ¤ë“œ ì„ íƒ
  getShardByRegion(regionCode: string): DatabaseShard {
    for (const [shardId, shard] of this.shards) {
      if (shard.containsRegion(regionCode)) {
        return shard;
      }
    }

    // ê¸°ë³¸ ìƒ¤ë“œ ë°˜í™˜
    return this.shards.get(this.defaultShard)!;
  }

  // ìœ„ì¹˜ ê¸°ë°˜ ìƒ¤ë“œ ì„ íƒ
  getShardByLocation(lat: number, lng: number): DatabaseShard {
    const regionCode = this.determineRegionByCoordinates(lat, lng);
    return this.getShardByRegion(regionCode);
  }

  // ì‚¬ìš©ì ê¸°ë°˜ ìƒ¤ë“œ ì„ íƒ
  async getShardByUser(userId: string): Promise<DatabaseShard> {
    const userRegion = await this.getUserRegion(userId);
    return this.getShardByRegion(userRegion);
  }

  // í¬ë¡œìŠ¤ ìƒ¤ë“œ ì¿¼ë¦¬
  async crossShardQuery<T>(
    query: string,
    params: any[],
    shardIds?: string[]
  ): Promise<T[]> {
    const targetShards = shardIds
      ? shardIds.map(id => this.shards.get(id)!).filter(Boolean)
      : Array.from(this.shards.values()).filter(shard => shard.isActive());

    const promises = targetShards.map(async shard => {
      try {
        return await shard.query<T>(query, params);
      } catch (error) {
        console.error(`Cross-shard query failed on ${shard.getId()}:`, error);
        return [];
      }
    });

    const results = await Promise.allSettled(promises);
    return results
      .filter((result): result is PromiseFulfilledResult<T[]> =>
        result.status === 'fulfilled')
      .flatMap(result => result.value);
  }

  private determineRegionByCoordinates(lat: number, lng: number): string {
    // ì œì£¼ë„ ì˜ì—­ ì²´í¬
    if (this.isInJeju(lat, lng)) {
      return 'KR-49';
    }

    // ë¶€ì‚° ì˜ì—­ ì²´í¬ (í–¥í›„ í™•ì¥)
    if (this.isInBusan(lat, lng)) {
      return 'KR-26';
    }

    // ê¸°ë³¸ê°’: ì œì£¼ë„
    return 'KR-49';
  }

  private isInJeju(lat: number, lng: number): boolean {
    // ì œì£¼ë„ ê²½ê³„ ë°•ìŠ¤ ì²´í¬
    return lat >= 33.1 && lat <= 33.6 && lng >= 126.1 && lng <= 126.9;
  }

  private isInBusan(lat: number, lng: number): boolean {
    // ë¶€ì‚° ê²½ê³„ ë°•ìŠ¤ ì²´í¬
    return lat >= 35.0 && lat <= 35.3 && lng >= 128.9 && lng <= 129.3;
  }

  private async getUserRegion(userId: string): Promise<string> {
    // ìºì‹œì—ì„œ ì‚¬ìš©ì ì§€ì—­ ì •ë³´ ì¡°íšŒ
    const cached = await cache.get(`user:region:${userId}`);
    if (cached) return cached;

    // ê¸°ë³¸ ìƒ¤ë“œì—ì„œ ì‚¬ìš©ì ì¡°íšŒ
    const defaultShard = this.shards.get(this.defaultShard)!;
    const user = await defaultShard.query(
      'SELECT region_code FROM users WHERE id = $1',
      [userId]
    );

    const regionCode = user[0]?.region_code || 'KR-49';
    await cache.set(`user:region:${userId}`, regionCode, 3600);

    return regionCode;
  }
}

// ë°ì´í„°ë² ì´ìŠ¤ ìƒ¤ë“œ í´ë˜ìŠ¤
class DatabaseShard {
  private master: Pool;
  private replicas: Pool[];
  private currentReplicaIndex = 0;

  constructor(private config: ShardConfig) {
    this.master = new Pool({
      host: config.primary.host,
      port: config.primary.port,
      database: config.primary.database,
      user: process.env.DB_USER,
      password: process.env.DB_PASSWORD,
      max: 20,
      idleTimeoutMillis: 30000
    });

    this.replicas = config.replicas.map(replica =>
      new Pool({
        host: replica.host,
        port: replica.port,
        database: replica.database,
        user: process.env.DB_USER,
        password: process.env.DB_PASSWORD,
        max: 30, // ì½ê¸° ì „ìš©ì´ë¯€ë¡œ ë” ë§ì€ ì—°ê²° í—ˆìš©
        idleTimeoutMillis: 30000
      })
    );
  }

  async query<T>(sql: string, params: any[] = []): Promise<T[]> {
    return this.read(sql, params);
  }

  async write<T>(sql: string, params: any[] = []): Promise<T> {
    const result = await this.master.query(sql, params);
    return result.rows[0];
  }

  async read<T>(sql: string, params: any[] = []): Promise<T[]> {
    // ì½ê¸° ì „ìš© ì¿¼ë¦¬ëŠ” ë ˆí”Œë¦¬ì¹´ë¡œ ë¼ìš´ë“œ ë¡œë¹ˆ
    const replica = this.getNextReplica();

    try {
      const result = await replica.query(sql, params);
      return result.rows;
    } catch (error) {
      // ë ˆí”Œë¦¬ì¹´ ì‹¤íŒ¨ ì‹œ ë§ˆìŠ¤í„°ë¡œ í´ë°±
      console.warn('Replica query failed, falling back to master:', error);
      const result = await this.master.query(sql, params);
      return result.rows;
    }
  }

  containsRegion(regionCode: string): boolean {
    return this.config.regions.some(region =>
      regionCode.startsWith(region)
    );
  }

  isActive(): boolean {
    return this.config.isActive;
  }

  getId(): string {
    return this.config.shardId;
  }

  private getNextReplica(): Pool {
    if (this.replicas.length === 0) {
      return this.master;
    }

    const replica = this.replicas[this.currentReplicaIndex];
    this.currentReplicaIndex = (this.currentReplicaIndex + 1) % this.replicas.length;
    return replica;
  }
}
```

#### 2. ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

```typescript
// ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë§¤ë‹ˆì €
export class DataMigrationManager {
  private sourceShards: Map<string, DatabaseShard>;
  private targetShards: Map<string, DatabaseShard>;

  constructor(
    sourceShards: Map<string, DatabaseShard>,
    targetShards: Map<string, DatabaseShard>
  ) {
    this.sourceShards = sourceShards;
    this.targetShards = targetShards;
  }

  // ì§€ì—­ë³„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
  async migrateRegionData(
    regionCode: string,
    fromShard: string,
    toShard: string
  ): Promise<MigrationResult> {
    const source = this.sourceShards.get(fromShard);
    const target = this.targetShards.get(toShard);

    if (!source || !target) {
      throw new Error(`Invalid shard configuration: ${fromShard} -> ${toShard}`);
    }

    const migrationPlan = await this.createMigrationPlan(regionCode, source);
    const result: MigrationResult = {
      regionCode,
      fromShard,
      toShard,
      startTime: new Date(),
      tables: [],
      totalRecords: 0,
      migratedRecords: 0,
      errors: []
    };

    try {
      // 1. í…Œì´ë¸”ë³„ ìˆœì°¨ ë§ˆì´ê·¸ë ˆì´ì…˜
      for (const table of migrationPlan.tables) {
        const tableResult = await this.migrateTable(
          table,
          regionCode,
          source,
          target
        );

        result.tables.push(tableResult);
        result.totalRecords += tableResult.totalRecords;
        result.migratedRecords += tableResult.migratedRecords;
      }

      // 2. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
      await this.validateMigration(regionCode, source, target);

      result.endTime = new Date();
      result.status = 'completed';

      return result;
    } catch (error) {
      result.endTime = new Date();
      result.status = 'failed';
      result.errors.push(error.message);

      // ë¡¤ë°± ìˆ˜í–‰
      await this.rollbackMigration(result);

      throw error;
    }
  }

  private async createMigrationPlan(
    regionCode: string,
    source: DatabaseShard
  ): Promise<MigrationPlan> {
    // ì˜ì¡´ì„± ìˆœì„œëŒ€ë¡œ í…Œì´ë¸” ì •ë ¬
    const tableOrder = [
      'users',
      'regions',
      'questions',
      'answers',
      'point_transactions',
      'user_badges',
      'notifications'
    ];

    const plan: MigrationPlan = {
      regionCode,
      tables: [],
      estimatedDuration: 0
    };

    for (const tableName of tableOrder) {
      const recordCount = await this.getTableRecordCount(
        tableName,
        regionCode,
        source
      );

      plan.tables.push({
        name: tableName,
        recordCount,
        estimatedDuration: this.estimateTableMigrationTime(tableName, recordCount),
        dependencies: this.getTableDependencies(tableName)
      });
    }

    plan.estimatedDuration = plan.tables.reduce(
      (total, table) => total + table.estimatedDuration,
      0
    );

    return plan;
  }

  private async migrateTable(
    table: TableMigrationInfo,
    regionCode: string,
    source: DatabaseShard,
    target: DatabaseShard
  ): Promise<TableMigrationResult> {
    const result: TableMigrationResult = {
      tableName: table.name,
      totalRecords: table.recordCount,
      migratedRecords: 0,
      startTime: new Date(),
      errors: []
    };

    const batchSize = this.getBatchSize(table.name);
    let offset = 0;

    try {
      while (offset < table.recordCount) {
        const batch = await this.extractBatch(
          table.name,
          regionCode,
          source,
          offset,
          batchSize
        );

        if (batch.length === 0) break;

        await this.loadBatch(table.name, target, batch);

        result.migratedRecords += batch.length;
        offset += batchSize;

        // ì§„í–‰ë¥  ë¡œê¹…
        const progress = (result.migratedRecords / result.totalRecords) * 100;
        console.log(`${table.name} migration progress: ${progress.toFixed(1)}%`);
      }

      result.endTime = new Date();
      result.status = 'completed';

      return result;
    } catch (error) {
      result.endTime = new Date();
      result.status = 'failed';
      result.errors.push(error.message);
      throw error;
    }
  }

  private async extractBatch(
    tableName: string,
    regionCode: string,
    source: DatabaseShard,
    offset: number,
    batchSize: number
  ): Promise<any[]> {
    const whereClause = this.getRegionFilterClause(tableName, regionCode);

    const query = `
      SELECT * FROM ${tableName}
      WHERE ${whereClause}
      ORDER BY created_at
      LIMIT $1 OFFSET $2
    `;

    return source.read(query, [batchSize, offset]);
  }

  private async loadBatch(
    tableName: string,
    target: DatabaseShard,
    batch: any[]
  ): Promise<void> {
    if (batch.length === 0) return;

    const columns = Object.keys(batch[0]);
    const placeholders = batch.map((_, index) =>
      `(${columns.map((_, colIndex) => `$${index * columns.length + colIndex + 1}`).join(', ')})`
    ).join(', ');

    const values = batch.flatMap(row => columns.map(col => row[col]));

    const query = `
      INSERT INTO ${tableName} (${columns.join(', ')})
      VALUES ${placeholders}
      ON CONFLICT (id) DO UPDATE SET
      ${columns.filter(col => col !== 'id').map(col => `${col} = EXCLUDED.${col}`).join(', ')}
    `;

    await target.write(query, values);
  }

  private getRegionFilterClause(tableName: string, regionCode: string): string {
    const regionFilters: Record<string, string> = {
      users: "region_code LIKE $1 || '%'",
      questions: "region_code LIKE $1 || '%'",
      answers: "question_id IN (SELECT id FROM questions WHERE region_code LIKE $1 || '%')",
      point_transactions: "user_id IN (SELECT id FROM users WHERE region_code LIKE $1 || '%')",
      user_badges: "user_id IN (SELECT id FROM users WHERE region_code LIKE $1 || '%')",
      notifications: "user_id IN (SELECT id FROM users WHERE region_code LIKE $1 || '%')"
    };

    return regionFilters[tableName] || "1=1";
  }
}
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### ì¿¼ë¦¬ ìµœì í™” ì „ëµ

#### 1. ì¸ë±ìŠ¤ ìµœì í™”

```sql
-- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë·° ìƒì„±
CREATE VIEW slow_queries AS
SELECT
    query,
    calls,
    total_time,
    mean_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
WHERE mean_time > 100  -- 100ms ì´ìƒ ì¿¼ë¦¬
ORDER BY mean_time DESC;

-- ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
CREATE VIEW index_usage AS
SELECT
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch,
    CASE
        WHEN idx_tup_read = 0 THEN 0
        ELSE (idx_tup_fetch::float / idx_tup_read::float) * 100
    END AS idx_usage_percent
FROM pg_stat_user_indexes
ORDER BY idx_usage_percent ASC;

-- í…Œì´ë¸” í¬ê¸° ë° ì¸ë±ìŠ¤ í¬ê¸° ëª¨ë‹ˆí„°ë§
CREATE VIEW table_sizes AS
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) AS index_size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- ìë™ ì¸ë±ìŠ¤ ì¶”ì²œ í•¨ìˆ˜
CREATE OR REPLACE FUNCTION recommend_indexes()
RETURNS TABLE(
    table_name TEXT,
    column_names TEXT,
    query_pattern TEXT,
    potential_benefit TEXT
) AS $$
BEGIN
    -- ìì£¼ ì‚¬ìš©ë˜ëŠ” WHERE ì ˆ íŒ¨í„´ ë¶„ì„
    RETURN QUERY
    WITH query_patterns AS (
        SELECT
            regexp_replace(query, '\$\d+', '?', 'g') AS normalized_query,
            calls,
            mean_time
        FROM pg_stat_statements
        WHERE query ILIKE '%WHERE%'
          AND calls > 10
          AND mean_time > 50
    )
    SELECT
        'questions'::TEXT,
        'region_code, status, created_at'::TEXT,
        'Frequent region + status queries'::TEXT,
        'High - 80% of queries use this pattern'::TEXT
    WHERE NOT EXISTS (
        SELECT 1 FROM pg_indexes
        WHERE tablename = 'questions'
        AND indexname LIKE '%region_status%'
    );
END;
$$ LANGUAGE plpgsql;
```

#### 2. ì—°ê²° í’€ ìµœì í™”

```typescript
// ê³ ê¸‰ ì—°ê²° í’€ ê´€ë¦¬ì
export class AdvancedConnectionPoolManager {
  private pools: Map<string, Pool> = new Map();
  private metrics: PoolMetrics = new PoolMetrics();

  constructor() {
    this.initializePools();
    this.startMetricsCollection();
  }

  private initializePools(): void {
    const poolConfigs: PoolConfig[] = [
      {
        name: 'read_heavy',
        config: {
          max: 50,           // ì½ê¸° ì§‘ì•½ì  ì‘ì—…ìš© í° í’€
          min: 10,
          acquireTimeoutMillis: 5000,
          createTimeoutMillis: 3000,
          idleTimeoutMillis: 30000,
          reapIntervalMillis: 1000,
          createRetryIntervalMillis: 200
        },
        usage: 'SELECT queries, analytics'
      },
      {
        name: 'write_critical',
        config: {
          max: 20,           // ì“°ê¸° ì‘ì—…ìš© ì œí•œëœ í’€
          min: 5,
          acquireTimeoutMillis: 2000,
          createTimeoutMillis: 1000,
          idleTimeoutMillis: 15000,
          reapIntervalMillis: 500,
          createRetryIntervalMillis: 100
        },
        usage: 'INSERT, UPDATE, DELETE queries'
      },
      {
        name: 'background_jobs',
        config: {
          max: 10,           // ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìš© ì†Œí˜• í’€
          min: 2,
          acquireTimeoutMillis: 10000,
          createTimeoutMillis: 5000,
          idleTimeoutMillis: 60000,
          reapIntervalMillis: 2000,
          createRetryIntervalMillis: 500
        },
        usage: 'Background processing, migrations'
      }
    ];

    poolConfigs.forEach(({ name, config }) => {
      const pool = new Pool({
        host: process.env.DB_HOST,
        port: 5432,
        database: process.env.DB_NAME,
        user: process.env.DB_USER,
        password: process.env.DB_PASSWORD,
        ...config,

        // ê³µí†µ ì„¤ì •
        ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,
        application_name: `dongne-${name}`,

        // ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        statement_timeout: name === 'read_heavy' ? 30000 : 10000,
        query_timeout: name === 'read_heavy' ? 30000 : 10000,

        // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        log: (message) => this.logPoolEvent(name, message)
      });

      // í’€ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§
      pool.on('connect', (client) => {
        this.metrics.recordConnection(name, 'connect');
      });

      pool.on('error', (err, client) => {
        this.metrics.recordError(name, err);
        console.error(`Pool ${name} error:`, err);
      });

      pool.on('remove', (client) => {
        this.metrics.recordConnection(name, 'remove');
      });

      this.pools.set(name, pool);
    });
  }

  // ì¿¼ë¦¬ íƒ€ì… ê¸°ë°˜ í’€ ì„ íƒ
  getPool(queryType: QueryType): Pool {
    switch (queryType) {
      case 'read':
      case 'analytics':
        return this.pools.get('read_heavy')!;

      case 'write':
      case 'transaction':
        return this.pools.get('write_critical')!;

      case 'background':
      case 'migration':
        return this.pools.get('background_jobs')!;

      default:
        return this.pools.get('read_heavy')!;
    }
  }

  // ë™ì  í’€ í¬ê¸° ì¡°ì •
  async adjustPoolSizes(): Promise<void> {
    for (const [name, pool] of this.pools) {
      const metrics = await this.getPoolMetrics(name);
      const newSize = this.calculateOptimalSize(metrics);

      if (newSize !== pool.options.max) {
        console.log(`Adjusting pool ${name} size: ${pool.options.max} -> ${newSize}`);
        await this.resizePool(name, newSize);
      }
    }
  }

  private calculateOptimalSize(metrics: PoolUsageMetrics): number {
    const { averageUsage, peakUsage, waitTime } = metrics;

    // ëŒ€ê¸° ì‹œê°„ì´ ê¸¸ë©´ í’€ í¬ê¸° ì¦ê°€
    if (waitTime > 1000) {
      return Math.min(peakUsage * 1.5, 100);
    }

    // ì‚¬ìš©ë¥ ì´ ë‚®ìœ¼ë©´ í’€ í¬ê¸° ê°ì†Œ
    if (averageUsage < 0.3) {
      return Math.max(peakUsage * 1.2, 5);
    }

    return peakUsage * 1.3;
  }

  private async resizePool(name: string, newSize: number): Promise<void> {
    const pool = this.pools.get(name);
    if (!pool) return;

    // ìƒˆ í’€ ìƒì„±
    const newPool = pool.clone({ max: newSize });

    // ê¸°ì¡´ í’€ê³¼ êµì²´
    this.pools.set(name, newPool);

    // ê¸°ì¡´ í’€ ì •ë¦¬ (ì§„í–‰ ì¤‘ì¸ ì¿¼ë¦¬ ì™„ë£Œ í›„)
    setTimeout(() => pool.end(), 30000);
  }

  private startMetricsCollection(): void {
    setInterval(async () => {
      await this.collectMetrics();
      await this.adjustPoolSizes();
    }, 60000); // 1ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
  }

  private async collectMetrics(): Promise<void> {
    for (const [name, pool] of this.pools) {
      const metrics = {
        totalConnections: pool.totalCount,
        idleConnections: pool.idleCount,
        waitingClients: pool.waitingCount,
        timestamp: new Date()
      };

      this.metrics.record(name, metrics);
    }
  }
}

// ì¿¼ë¦¬ ìµœì í™” ë¶„ì„ê¸°
export class QueryOptimizer {
  private slowQueries: Map<string, QueryStats> = new Map();

  async analyzeQuery(sql: string, params: any[], executionTime: number): Promise<QueryAnalysis> {
    const normalizedQuery = this.normalizeQuery(sql);
    const stats = this.getOrCreateStats(normalizedQuery);

    stats.executions++;
    stats.totalTime += executionTime;
    stats.avgTime = stats.totalTime / stats.executions;

    if (executionTime > stats.maxTime) {
      stats.maxTime = executionTime;
    }

    // ëŠë¦° ì¿¼ë¦¬ ë¶„ì„
    if (executionTime > 1000) { // 1ì´ˆ ì´ìƒ
      return this.analyzeslowQuery(sql, params, executionTime);
    }

    return { needsOptimization: false };
  }

  private async analyzeslowQuery(
    sql: string,
    params: any[],
    executionTime: number
  ): Promise<QueryAnalysis> {
    const analysis: QueryAnalysis = {
      needsOptimization: true,
      executionTime,
      recommendations: []
    };

    // 1. EXPLAIN ANALYZE ì‹¤í–‰
    try {
      const explainResult = await this.runExplain(sql, params);
      analysis.explainPlan = explainResult;

      // 2. ì‹¤í–‰ ê³„íš ë¶„ì„
      if (this.hasSequentialScan(explainResult)) {
        analysis.recommendations.push({
          type: 'missing_index',
          description: 'Sequential scan detected. Consider adding indexes.',
          impact: 'high'
        });
      }

      if (this.hasHighCost(explainResult)) {
        analysis.recommendations.push({
          type: 'query_rewrite',
          description: 'High execution cost. Consider query optimization.',
          impact: 'medium'
        });
      }

      if (this.hasNestedLoop(explainResult)) {
        analysis.recommendations.push({
          type: 'join_optimization',
          description: 'Nested loop detected. Check join conditions.',
          impact: 'high'
        });
      }

    } catch (error) {
      console.error('Query analysis failed:', error);
    }

    return analysis;
  }

  private normalizeQuery(sql: string): string {
    return sql
      .replace(/\$\d+/g, '?')           // íŒŒë¼ë¯¸í„° ì •ê·œí™”
      .replace(/\s+/g, ' ')            // ê³µë°± ì •ê·œí™”
      .replace(/--.*$/gm, '')          // ì£¼ì„ ì œê±°
      .trim()
      .toLowerCase();
  }

  private getOrCreateStats(normalizedQuery: string): QueryStats {
    if (!this.slowQueries.has(normalizedQuery)) {
      this.slowQueries.set(normalizedQuery, {
        query: normalizedQuery,
        executions: 0,
        totalTime: 0,
        avgTime: 0,
        maxTime: 0,
        firstSeen: new Date(),
        lastSeen: new Date()
      });
    }

    const stats = this.slowQueries.get(normalizedQuery)!;
    stats.lastSeen = new Date();
    return stats;
  }

  // ì •ê¸°ì ì¸ ìµœì í™” ë¦¬í¬íŠ¸ ìƒì„±
  generateOptimizationReport(): OptimizationReport {
    const topSlowQueries = Array.from(this.slowQueries.values())
      .sort((a, b) => b.avgTime - a.avgTime)
      .slice(0, 10);

    const recommendations = topSlowQueries.map(query => ({
      query: query.query.substring(0, 100) + '...',
      avgTime: query.avgTime,
      executions: query.executions,
      totalImpact: query.avgTime * query.executions,
      recommendations: this.generateQueryRecommendations(query)
    }));

    return {
      reportDate: new Date(),
      topSlowQueries: recommendations,
      summary: {
        totalQueriesAnalyzed: this.slowQueries.size,
        averageExecutionTime: this.calculateAverageExecutionTime(),
        totalOptimizationPotential: recommendations.reduce(
          (sum, r) => sum + r.totalImpact, 0
        )
      }
    };
  }

  private generateQueryRecommendations(stats: QueryStats): string[] {
    const recommendations: string[] = [];

    if (stats.avgTime > 5000) {
      recommendations.push('Consider breaking down into smaller queries');
    }

    if (stats.executions > 1000) {
      recommendations.push('High frequency query - prime candidate for caching');
    }

    if (stats.query.includes('order by') && !stats.query.includes('limit')) {
      recommendations.push('Add LIMIT clause to ORDER BY queries');
    }

    return recommendations;
  }
}
```

---

## ğŸ’¾ ë°±ì—… ë° ë³µêµ¬

### ìë™í™”ëœ ë°±ì—… ì‹œìŠ¤í…œ

#### 1. í¬ê´„ì  ë°±ì—… ì „ëµ

```typescript
// ë°±ì—… ë§¤ë‹ˆì €
export class ComprehensiveBackupManager {
  private backupSchedules: Map<string, BackupSchedule> = new Map();
  private s3Client: S3Client;
  private notificationService: NotificationService;

  constructor() {
    this.s3Client = new S3Client({ region: 'ap-northeast-2' });
    this.notificationService = new NotificationService();
    this.initializeSchedules();
  }

  private initializeSchedules(): void {
    const schedules: BackupScheduleConfig[] = [
      {
        name: 'postgresql_full',
        type: 'full',
        source: 'postgresql',
        cron: '0 2 * * 0',        // ì£¼ì¼ ìƒˆë²½ 2ì‹œ (ì¼ìš”ì¼)
        retention: 12,             // 12ì£¼ ë³´ê´€
        compression: true,
        encryption: true,
        priority: 'high'
      },
      {
        name: 'postgresql_incremental',
        type: 'incremental',
        source: 'postgresql',
        cron: '0 2 * * 1-6',      // í‰ì¼ ìƒˆë²½ 2ì‹œ
        retention: 4,              // 4ì£¼ ë³´ê´€
        compression: true,
        encryption: true,
        priority: 'high'
      },
      {
        name: 'redis_snapshot',
        type: 'snapshot',
        source: 'redis',
        cron: '0 */6 * * *',      // 6ì‹œê°„ë§ˆë‹¤
        retention: 7,              // 7ì¼ ë³´ê´€
        compression: true,
        encryption: false,
        priority: 'medium'
      },
      {
        name: 'elasticsearch_snapshot',
        type: 'snapshot',
        source: 'elasticsearch',
        cron: '0 3 * * *',        // ë§¤ì¼ ìƒˆë²½ 3ì‹œ
        retention: 30,             // 30ì¼ ë³´ê´€
        compression: true,
        encryption: true,
        priority: 'medium'
      },
      {
        name: 's3_files_sync',
        type: 'sync',
        source: 's3',
        cron: '0 4 * * *',        // ë§¤ì¼ ìƒˆë²½ 4ì‹œ
        retention: 90,             // 90ì¼ ë³´ê´€
        compression: false,
        encryption: true,
        priority: 'low'
      }
    ];

    schedules.forEach(config => {
      const schedule = cron.schedule(config.cron, async () => {
        await this.executeBackup(config);
      }, { scheduled: false });

      this.backupSchedules.set(config.name, {
        config,
        schedule,
        lastRun: null,
        nextRun: null,
        status: 'pending'
      });
    });

    // ìŠ¤ì¼€ì¤„ ì‹œì‘
    this.startAllSchedules();
  }

  async executeBackup(config: BackupScheduleConfig): Promise<BackupResult> {
    const startTime = new Date();
    const backupId = `${config.name}_${startTime.toISOString().replace(/[:.]/g, '-')}`;

    console.log(`Starting backup: ${backupId}`);

    const result: BackupResult = {
      id: backupId,
      config,
      startTime,
      status: 'running',
      logs: []
    };

    try {
      switch (config.source) {
        case 'postgresql':
          await this.backupPostgreSQL(config, result);
          break;
        case 'redis':
          await this.backupRedis(config, result);
          break;
        case 'elasticsearch':
          await this.backupElasticsearch(config, result);
          break;
        case 's3':
          await this.syncS3Files(config, result);
          break;
        default:
          throw new Error(`Unsupported backup source: ${config.source}`);
      }

      result.endTime = new Date();
      result.status = 'completed';
      result.duration = result.endTime.getTime() - result.startTime.getTime();

      // ë°±ì—… ì™„ë£Œ ì•Œë¦¼
      await this.notifyBackupCompletion(result);

      // ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
      await this.cleanupOldBackups(config);

      return result;

    } catch (error) {
      result.endTime = new Date();
      result.status = 'failed';
      result.error = error.message;
      result.logs.push(`ERROR: ${error.message}`);

      // ì‹¤íŒ¨ ì•Œë¦¼
      await this.notifyBackupFailure(result);

      throw error;
    }
  }

  private async backupPostgreSQL(
    config: BackupScheduleConfig,
    result: BackupResult
  ): Promise<void> {
    const timestamp = result.startTime.toISOString().replace(/[:.]/g, '-');
    const backupFileName = `postgresql_${config.type}_${timestamp}`;
    const localPath = `/tmp/${backupFileName}`;
    const s3Key = `backups/postgresql/${config.type}/${backupFileName}`;

    try {
      if (config.type === 'full') {
        // pg_dumpë¥¼ ì´ìš©í•œ ì „ì²´ ë°±ì—…
        const dumpCommand = [
          'pg_dump',
          process.env.DATABASE_URL!,
          '--format=custom',
          '--compress=9',
          '--verbose',
          '--no-password',
          '--file', localPath
        ];

        await this.executeCommand(dumpCommand, result);

      } else if (config.type === 'incremental') {
        // WAL íŒŒì¼ ê¸°ë°˜ ì¦ë¶„ ë°±ì—…
        await this.backupWALFiles(localPath, result);
      }

      // ì••ì¶• (í•„ìš”ì‹œ)
      let finalPath = localPath;
      if (config.compression && config.type === 'full') {
        finalPath = await this.compressFile(localPath, result);
      }

      // ì•”í˜¸í™” (í•„ìš”ì‹œ)
      if (config.encryption) {
        finalPath = await this.encryptFile(finalPath, result);
      }

      // S3 ì—…ë¡œë“œ
      const uploadResult = await this.uploadToS3(finalPath, s3Key, result);
      result.s3Location = uploadResult.Location;
      result.size = uploadResult.Size;

      // ë¡œì»¬ íŒŒì¼ ì •ë¦¬
      await fs.unlink(finalPath);
      if (finalPath !== localPath) {
        await fs.unlink(localPath);
      }

      result.logs.push(`PostgreSQL ${config.type} backup completed successfully`);

    } catch (error) {
      result.logs.push(`PostgreSQL backup failed: ${error.message}`);
      throw error;
    }
  }

  private async backupRedis(
    config: BackupScheduleConfig,
    result: BackupResult
  ): Promise<void> {
    const timestamp = result.startTime.toISOString().replace(/[:.]/g, '-');
    const backupFileName = `redis_snapshot_${timestamp}.rdb`;
    const localPath = `/tmp/${backupFileName}`;
    const s3Key = `backups/redis/${backupFileName}`;

    try {
      // Redis BGSAVE ëª…ë ¹ìœ¼ë¡œ ìŠ¤ëƒ…ìƒ· ìƒì„±
      const redis = new Redis(process.env.REDIS_URL!);

      result.logs.push('Initiating Redis BGSAVE...');
      await redis.bgsave();

      // BGSAVE ì™„ë£Œ ëŒ€ê¸°
      let saveInProgress = true;
      while (saveInProgress) {
        const info = await redis.info('persistence');
        saveInProgress = info.includes('rdb_bgsave_in_progress:1');

        if (saveInProgress) {
          await new Promise(resolve => setTimeout(resolve, 1000));
          result.logs.push('Waiting for BGSAVE to complete...');
        }
      }

      // RDB íŒŒì¼ ë³µì‚¬
      const rdbPath = '/data/dump.rdb'; // Redis ë°ì´í„° ë””ë ‰í† ë¦¬
      await fs.copyFile(rdbPath, localPath);

      // ì••ì¶•
      if (config.compression) {
        const compressedPath = await this.compressFile(localPath, result);
        await fs.unlink(localPath);
        localPath = compressedPath;
      }

      // S3 ì—…ë¡œë“œ
      const uploadResult = await this.uploadToS3(localPath, s3Key, result);
      result.s3Location = uploadResult.Location;
      result.size = uploadResult.Size;

      // ì •ë¦¬
      await fs.unlink(localPath);
      await redis.disconnect();

      result.logs.push('Redis backup completed successfully');

    } catch (error) {
      result.logs.push(`Redis backup failed: ${error.message}`);
      throw error;
    }
  }

  private async backupElasticsearch(
    config: BackupScheduleConfig,
    result: BackupResult
  ): Promise<void> {
    const timestamp = result.startTime.toISOString().replace(/[:.]/g, '-');
    const snapshotName = `snapshot_${timestamp}`;
    const repositoryName = 'dongne_backup_repo';

    try {
      const esClient = new Client({
        node: process.env.ELASTICSEARCH_URL!
      });

      // ìŠ¤ëƒ…ìƒ· ë ˆí¬ì§€í† ë¦¬ í™•ì¸/ìƒì„±
      try {
        await esClient.snapshot.getRepository({
          repository: repositoryName
        });
      } catch (error) {
        // ë ˆí¬ì§€í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        await esClient.snapshot.createRepository({
          repository: repositoryName,
          body: {
            type: 's3',
            settings: {
              bucket: process.env.BACKUP_S3_BUCKET!,
              base_path: 'elasticsearch',
              region: 'ap-northeast-2'
            }
          }
        });
        result.logs.push('Created snapshot repository');
      }

      // ìŠ¤ëƒ…ìƒ· ìƒì„±
      result.logs.push(`Creating snapshot: ${snapshotName}`);
      const snapshotResponse = await esClient.snapshot.create({
        repository: repositoryName,
        snapshot: snapshotName,
        body: {
          indices: 'questions,users,hashtags',
          ignore_unavailable: true,
          include_global_state: false,
          metadata: {
            created_by: 'dongne_backup_manager',
            created_at: result.startTime.toISOString()
          }
        }
      });

      // ìŠ¤ëƒ…ìƒ· ì™„ë£Œ ëŒ€ê¸°
      let snapshotComplete = false;
      while (!snapshotComplete) {
        const statusResponse = await esClient.snapshot.status({
          repository: repositoryName,
          snapshot: snapshotName
        });

        const snapshot = statusResponse.body.snapshots[0];
        if (snapshot && snapshot.state === 'SUCCESS') {
          snapshotComplete = true;
          result.logs.push('Snapshot completed successfully');
        } else if (snapshot && snapshot.state === 'FAILED') {
          throw new Error(`Snapshot failed: ${snapshot.state}`);
        } else {
          await new Promise(resolve => setTimeout(resolve, 5000));
          result.logs.push('Waiting for snapshot to complete...');
        }
      }

      // ìŠ¤ëƒ…ìƒ· ì •ë³´ ì €ì¥
      const snapshotInfo = await esClient.snapshot.get({
        repository: repositoryName,
        snapshot: snapshotName
      });

      result.size = snapshotInfo.body.snapshots[0].stats?.total?.size_in_bytes || 0;
      result.s3Location = `s3://${process.env.BACKUP_S3_BUCKET}/elasticsearch/${snapshotName}`;

      result.logs.push('Elasticsearch backup completed successfully');

    } catch (error) {
      result.logs.push(`Elasticsearch backup failed: ${error.message}`);
      throw error;
    }
  }

  // ë³µêµ¬ ë§¤ë‹ˆì €
  async restoreFromBackup(
    source: 'postgresql' | 'redis' | 'elasticsearch',
    backupId: string,
    options: RestoreOptions = {}
  ): Promise<RestoreResult> {
    const startTime = new Date();
    const restoreId = `restore_${backupId}_${startTime.toISOString().replace(/[:.]/g, '-')}`;

    const result: RestoreResult = {
      id: restoreId,
      backupId,
      source,
      startTime,
      status: 'running',
      logs: []
    };

    try {
      switch (source) {
        case 'postgresql':
          await this.restorePostgreSQL(backupId, options, result);
          break;
        case 'redis':
          await this.restoreRedis(backupId, options, result);
          break;
        case 'elasticsearch':
          await this.restoreElasticsearch(backupId, options, result);
          break;
        default:
          throw new Error(`Unsupported restore source: ${source}`);
      }

      result.endTime = new Date();
      result.status = 'completed';
      result.duration = result.endTime.getTime() - result.startTime.getTime();

      // ë³µêµ¬ ì™„ë£Œ ì•Œë¦¼
      await this.notifyRestoreCompletion(result);

      return result;

    } catch (error) {
      result.endTime = new Date();
      result.status = 'failed';
      result.error = error.message;
      result.logs.push(`ERROR: ${error.message}`);

      // ì‹¤íŒ¨ ì•Œë¦¼
      await this.notifyRestoreFailure(result);

      throw error;
    }
  }

  private async restorePostgreSQL(
    backupId: string,
    options: RestoreOptions,
    result: RestoreResult
  ): Promise<void> {
    try {
      // ë°±ì—… íŒŒì¼ ë‹¤ìš´ë¡œë“œ
      const backupInfo = await this.getBackupInfo(backupId);
      const localPath = `/tmp/restore_${backupId}`;

      result.logs.push('Downloading backup from S3...');
      await this.downloadFromS3(backupInfo.s3Location, localPath);

      // ë³µí˜¸í™” (í•„ìš”ì‹œ)
      let restorePath = localPath;
      if (backupInfo.encrypted) {
        restorePath = await this.decryptFile(localPath, result);
      }

      // ì••ì¶• í•´ì œ (í•„ìš”ì‹œ)
      if (backupInfo.compressed) {
        restorePath = await this.decompressFile(restorePath, result);
      }

      // ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬
      if (options.targetDatabase) {
        // ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³µêµ¬
        await this.createDatabase(options.targetDatabase);
        result.logs.push(`Created target database: ${options.targetDatabase}`);
      }

      const targetDb = options.targetDatabase || process.env.DB_NAME;
      const restoreCommand = [
        'pg_restore',
        '--host', process.env.DB_HOST!,
        '--port', '5432',
        '--username', process.env.DB_USER!,
        '--dbname', targetDb,
        '--verbose',
        '--no-password'
      ];

      if (options.clean) {
        restoreCommand.push('--clean');
      }

      if (options.createTables) {
        restoreCommand.push('--create');
      }

      restoreCommand.push(restorePath);

      await this.executeCommand(restoreCommand, result);

      // ì •ë¦¬
      await fs.unlink(localPath);
      if (restorePath !== localPath) {
        await fs.unlink(restorePath);
      }

      result.logs.push('PostgreSQL restore completed successfully');

    } catch (error) {
      result.logs.push(`PostgreSQL restore failed: ${error.message}`);
      throw error;
    }
  }

  // ë°±ì—… ìƒíƒœ ëª¨ë‹ˆí„°ë§
  getBackupStatus(): BackupStatusReport {
    const schedules = Array.from(this.backupSchedules.values());

    return {
      totalSchedules: schedules.length,
      activeSchedules: schedules.filter(s => s.schedule.running).length,
      lastSuccessfulBackups: schedules.map(s => ({
        name: s.config.name,
        lastRun: s.lastRun,
        status: s.status,
        nextRun: s.nextRun
      })),
      upcomingBackups: this.getUpcomingBackups(),
      storageUsage: this.calculateStorageUsage()
    };
  }

  private async cleanupOldBackups(config: BackupScheduleConfig): Promise<void> {
    try {
      const prefix = `backups/${config.source}/${config.type}/`;
      const objects = await this.s3Client.send(new ListObjectsV2Command({
        Bucket: process.env.BACKUP_S3_BUCKET!,
        Prefix: prefix
      }));

      if (!objects.Contents) return;

      // ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ë¶€í„°)
      const sortedObjects = objects.Contents
        .sort((a, b) => (b.LastModified?.getTime() || 0) - (a.LastModified?.getTime() || 0));

      // ë³´ê´€ ê¸°ê°„ì„ ì´ˆê³¼í•œ ê°ì²´ ì‚­ì œ
      const objectsToDelete = sortedObjects.slice(config.retention);

      if (objectsToDelete.length > 0) {
        await this.s3Client.send(new DeleteObjectsCommand({
          Bucket: process.env.BACKUP_S3_BUCKET!,
          Delete: {
            Objects: objectsToDelete.map(obj => ({ Key: obj.Key! }))
          }
        }));

        console.log(`Cleaned up ${objectsToDelete.length} old backups for ${config.name}`);
      }
    } catch (error) {
      console.error(`Failed to cleanup old backups for ${config.name}:`, error);
    }
  }
}
```

ì´ ë°ì´í„°ë² ì´ìŠ¤ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œëŠ” "ë™ë„¤ë¬¼ì–´ë´" ì„œë¹„ìŠ¤ì˜ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í¬ê´„ì ì¸ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. PostgreSQL, Redis, Elasticsearchì˜ í†µí•©ì  í™œìš©ê³¼ ë©€í‹°ë¦¬ì „ í™•ì¥ì„ ê³ ë ¤í•œ ì„¤ê³„ë¡œ ì„œë¹„ìŠ¤ì˜ ì„±ì¥ê³¼ í•¨ê»˜ í™•ì¥í•  ìˆ˜ ìˆëŠ” ê²¬ê³ í•œ ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤.